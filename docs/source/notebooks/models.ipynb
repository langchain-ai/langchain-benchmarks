{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cb90d8-e6a1-4c89-9cde-0e6c0a28f5c0",
   "metadata": {},
   "source": [
    "# Model Registry\n",
    "\n",
    "LangChain Benchmark includes a model registry to make it easier to run benchmarks across different models.\n",
    "\n",
    "If you see a model that you want to use and it's missing, please open a PR to add it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31831289-51fb-4ee5-98f3-0476cf11b187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_benchmarks import model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaed190d-fa4b-4445-9bfb-0e784e2a083b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                  </th><th>Type  </th><th>Provider  </th><th>Description                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>gpt-3.5-turbo-1106    </td><td>chat  </td><td>openai    </td><td>The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.</td></tr>\n",
       "<tr><td>gpt-3.5-turbo         </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                            </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-16k     </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                            </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-instruct</td><td>llm   </td><td>openai    </td><td>Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.                                                                 </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-0613    </td><td>chat  </td><td>openai    </td><td>Legacy Snapshot of gpt-3.5-turbo from June 13th 2023. Will be deprecated on June 13, 2024.                                                                                         </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-16k-0613</td><td>chat  </td><td>openai    </td><td>Legacy Snapshot of gpt-3.5-16k-turbo from June 13th 2023. Will be deprecated on June 13, 2024.                                                                                     </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-0301    </td><td>chat  </td><td>openai    </td><td>Legacy Snapshot of gpt-3.5-turbo from March 1st 2023. Will be deprecated on June 13th 2024.                                                                                        </td></tr>\n",
       "<tr><td>text-davinci-003      </td><td>llm   </td><td>openai    </td><td>Legacy Can do language tasks with better quality and consistency than the curie, babbage, or ada models. Will be deprecated on Jan 4th 2024.                                       </td></tr>\n",
       "<tr><td>text-davinci-002      </td><td>llm   </td><td>openai    </td><td>Legacy Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning. Will be deprecated on Jan 4th 2024.                     </td></tr>\n",
       "<tr><td>code-davinci-002      </td><td>llm   </td><td>openai    </td><td>Legacy Optimized for code-completion tasks. Will be deprecated on Jan 4th 2024.                                                                                                    </td></tr>\n",
       "<tr><td>llama-v2-7b-chat-fw   </td><td>chat  </td><td>fireworks </td><td>7b parameter LlamaChat model                                                                                                                                                       </td></tr>\n",
       "<tr><td>llama-v2-13b-chat-fw  </td><td>chat  </td><td>fireworks </td><td>13b parameter LlamaChat model                                                                                                                                                      </td></tr>\n",
       "<tr><td>llama-v2-70b-chat-fw  </td><td>chat  </td><td>fireworks </td><td>70b parameter LlamaChat model                                                                                                                                                      </td></tr>\n",
       "<tr><td>claude-2              </td><td>chat  </td><td>anthropic </td><td>Superior performance on tasks that require complex reasoning                                                                                                                       </td></tr>\n",
       "<tr><td>claude-2.1            </td><td>chat  </td><td>anthropic </td><td>Same performance as Claude 2, plus significant reduction in model hallucination rates                                                                                              </td></tr>\n",
       "<tr><td>claude-instant-1.2    </td><td>chat  </td><td>anthropic </td><td>low-latency, high throughput.                                                                                                                                                      </td></tr>\n",
       "<tr><td>claude-instant-1      </td><td>chat  </td><td>anthropic </td><td>low-latency, high throughput.                                                                                                                                                      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ModelRegistry(registered_models=[RegisteredModel(name='gpt-3.5-turbo-1106', provider='openai', description='The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.', params={'model': 'gpt-3.5-turbo-1106'}, type='chat', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo'}, type='chat', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo-16k', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo-16k'}, type='chat', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo-instruct', provider='openai', description='Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.', params={'model': 'gpt-3.5-turbo-instruct'}, type='llm', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo-0613', provider='openai', description='Legacy Snapshot of gpt-3.5-turbo from June 13th 2023. Will be deprecated on June 13, 2024.', params={'model': 'gpt-3.5-turbo-0613'}, type='chat', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo-16k-0613', provider='openai', description='Legacy Snapshot of gpt-3.5-16k-turbo from June 13th 2023. Will be deprecated on June 13, 2024.', params={'model': 'gpt-3.5-turbo-16k-0613'}, type='chat', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo-0301', provider='openai', description='Legacy Snapshot of gpt-3.5-turbo from March 1st 2023. Will be deprecated on June 13th 2024.', params={'model': 'gpt-3.5-turbo-0301'}, type='chat', path=None, url=None), RegisteredModel(name='text-davinci-003', provider='openai', description='Legacy Can do language tasks with better quality and consistency than the curie, babbage, or ada models. Will be deprecated on Jan 4th 2024.', params={'model': 'text-davinci-003'}, type='llm', path=None, url=None), RegisteredModel(name='text-davinci-002', provider='openai', description='Legacy Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning. Will be deprecated on Jan 4th 2024.', params={'model': 'text-davinci-002'}, type='llm', path=None, url=None), RegisteredModel(name='code-davinci-002', provider='openai', description='Legacy Optimized for code-completion tasks. Will be deprecated on Jan 4th 2024.', params={'model': 'code-davinci-002'}, type='llm', path=None, url=None), RegisteredModel(name='llama-v2-7b-chat-fw', provider='fireworks', description='7b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-7b-chat'}, type='chat', path=None, url=None), RegisteredModel(name='llama-v2-13b-chat-fw', provider='fireworks', description='13b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-13b-chat'}, type='chat', path=None, url=None), RegisteredModel(name='llama-v2-70b-chat-fw', provider='fireworks', description='70b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-70b-chat'}, type='chat', path=None, url=None), RegisteredModel(name='claude-2', provider='anthropic', description='Superior performance on tasks that require complex reasoning', params={'model': 'claude-2'}, type='chat', path=None, url=None), RegisteredModel(name='claude-2.1', provider='anthropic', description='Same performance as Claude 2, plus significant reduction in model hallucination rates', params={'model': 'claude-2.1'}, type='chat', path=None, url=None), RegisteredModel(name='claude-instant-1.2', provider='anthropic', description='low-latency, high throughput.', params={'model': 'claude-instant-1.2'}, type='chat', path=None, url=None), RegisteredModel(name='claude-instant-1', provider='anthropic', description='low-latency, high throughput.', params={'model': 'claude-instant-1'}, type='chat', path=None, url=None)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974b4f9-575c-4907-97eb-7334ef5f1d8e",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Registry supports indexing by position. This ordering may change as more models get added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64bfc631-1f1e-4cf4-8636-b8be7b46fef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>name       </td><td>gpt-3.5-turbo-1106                                                                                                                                                                 </td></tr>\n",
       "<tr><td>type       </td><td>chat                                                                                                                                                                               </td></tr>\n",
       "<tr><td>provider   </td><td>openai                                                                                                                                                                             </td></tr>\n",
       "<tr><td>description</td><td>The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.</td></tr>\n",
       "<tr><td>model_path </td><td>langchain.chat_models.openai.ChatOpenAI                                                                                                                                            </td></tr>\n",
       "<tr><td>url        </td><td><a href=\"langchain.chat_models.openai.ChatOpenAI\" target=\"_blank\" rel=\"noopener\">ModelPage</a>                                                                                     </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RegisteredModel(name='gpt-3.5-turbo-1106', provider='openai', description='The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.', params={'model': 'gpt-3.5-turbo-1106'}, type='chat', path=None, url=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_model = model_registry[0]\n",
    "registered_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150df8d-11a5-4e83-bc1c-b34119f75783",
   "metadata": {},
   "source": [
    "Can also index by model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267e746b-f13e-4484-bcbb-ed5dfbacae67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>name       </td><td>gpt-3.5-turbo                                                                                 </td></tr>\n",
       "<tr><td>type       </td><td>chat                                                                                          </td></tr>\n",
       "<tr><td>provider   </td><td>openai                                                                                        </td></tr>\n",
       "<tr><td>description</td><td>Currently points to gpt-3.5-turbo-0613.                                                       </td></tr>\n",
       "<tr><td>model_path </td><td>langchain.chat_models.openai.ChatOpenAI                                                       </td></tr>\n",
       "<tr><td>url        </td><td><a href=\"langchain.chat_models.openai.ChatOpenAI\" target=\"_blank\" rel=\"noopener\">ModelPage</a></td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RegisteredModel(name='gpt-3.5-turbo', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo'}, type='chat', path=None, url=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry[\"gpt-3.5-turbo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26404672-0832-47be-bc7e-7f74116f6909",
   "metadata": {},
   "source": [
    "## Use the model\n",
    "\n",
    "To use the models, make sure that you have credentials set up. Most models take either an API key as part of the initializer or will use any ENV variables that might be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3604d49e-afbe-48ad-ac10-1e538b1ad376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model_registry[\"gpt-3.5-turbo\"].get_model(model_params={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdece532-9843-427a-a10b-4545ed4ec151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I am an AI language model developed by OpenAI, and I don't have a personal name. You can simply refer to me as OpenAI Assistant. How can I assist you today?\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello! what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095ad4c7-a796-4d6d-bf1f-706799c1f743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model_registry[\"claude-2.1\"].get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8c656d7-d17a-4dd9-bfdb-34da26f1ba57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Hello! My name is Claude.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello! what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a193bca5-67d4-4e83-841b-7c28089d76c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model_registry[\"llama-v2-7b-chat-fw\"].get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac4c3216-d343-4346-b089-deceef91b334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! *smiling* My name is Assistant, and I'm here to help you with any questions or tasks you may have. It's important to me to provide respectful and socially unbiased responses, and I'm glad you're here to chat with me! Is there something specific you'd like to talk about or ask? ðŸ˜Š\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello! what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5769e2a6-55ae-41b6-b0c9-2fce59a7a409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMy name is [Name].'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_registry[\"text-davinci-003\"].get_model()\n",
    "model.invoke(\"hello! what is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34801c48-83ed-4ada-b85b-aa3b8cfce31b",
   "metadata": {},
   "source": [
    "## Slicing\n",
    "\n",
    "Slicing notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db40d4da-dc70-4e6d-b7e8-61de1e15ed2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name              </th><th>Type  </th><th>Provider  </th><th>Description                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>gpt-3.5-turbo-1106</td><td>chat  </td><td>openai    </td><td>The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.</td></tr>\n",
       "<tr><td>gpt-3.5-turbo     </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                            </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-16k </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ModelRegistry(registered_models=[RegisteredModel(name='gpt-3.5-turbo-1106', provider='openai', description='The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.', params={'model': 'gpt-3.5-turbo-1106'}, type='chat', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo'}, type='chat', path=None, url=None), RegisteredModel(name='gpt-3.5-turbo-16k', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo-16k'}, type='chat', path=None, url=None)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0260af-920f-4512-9273-6f7662369ec5",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "\n",
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9874846a-52f3-4921-b1ed-0858521bb9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                </th><th>Type  </th><th>Provider  </th><th>Description                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>llama-v2-7b-chat-fw </td><td>chat  </td><td>fireworks </td><td>7b parameter LlamaChat model </td></tr>\n",
       "<tr><td>llama-v2-13b-chat-fw</td><td>chat  </td><td>fireworks </td><td>13b parameter LlamaChat model</td></tr>\n",
       "<tr><td>llama-v2-70b-chat-fw</td><td>chat  </td><td>fireworks </td><td>70b parameter LlamaChat model</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ModelRegistry(registered_models=[RegisteredModel(name='llama-v2-7b-chat-fw', provider='fireworks', description='7b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-7b-chat'}, type='chat', path=None, url=None), RegisteredModel(name='llama-v2-13b-chat-fw', provider='fireworks', description='13b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-13b-chat'}, type='chat', path=None, url=None), RegisteredModel(name='llama-v2-70b-chat-fw', provider='fireworks', description='70b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-70b-chat'}, type='chat', path=None, url=None)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry.filter(provider=\"fireworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59baa6f-c3c1-4e37-919c-f9e70feb9101",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "\n",
    "You can iterate through the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb531591-f46b-4745-ae67-4dfd6217ec5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-0301\n",
      "text-davinci-003\n",
      "text-davinci-002\n",
      "code-davinci-002\n",
      "llama-v2-7b-chat-fw\n",
      "llama-v2-13b-chat-fw\n",
      "llama-v2-70b-chat-fw\n",
      "claude-2\n",
      "claude-2.1\n",
      "claude-instant-1.2\n",
      "claude-instant-1\n"
     ]
    }
   ],
   "source": [
    "for registered_model in model_registry:\n",
    "    print(registered_model.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
