{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cb90d8-e6a1-4c89-9cde-0e6c0a28f5c0",
   "metadata": {},
   "source": [
    "# Model Registry\n",
    "\n",
    "LangChain Benchmark includes a model registry to make it easier to run benchmarks across different models.\n",
    "\n",
    "If you see a model that you want to use and it's missing, please open a PR to add it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31831289-51fb-4ee5-98f3-0476cf11b187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_benchmarks import model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaed190d-fa4b-4445-9bfb-0e784e2a083b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                  </th><th>Type  </th><th>Provider  </th><th>Description                                                                                                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>gpt-3.5-turbo-1106    </td><td>chat  </td><td>openai    </td><td>The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Learn more.</td></tr>\n",
       "<tr><td>gpt-3.5-turbo         </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                                        </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-16k     </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                                        </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-instruct</td><td>llm   </td><td>openai    </td><td>Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.                                                                             </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-0613    </td><td>chat  </td><td>openai    </td><td>Legacy Snapshot of gpt-3.5-turbo from June 13th 2023. Will be deprecated on June 13, 2024.                                                                                                     </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-16k-0613</td><td>chat  </td><td>openai    </td><td>Legacy Snapshot of gpt-3.5-16k-turbo from June 13th 2023. Will be deprecated on June 13, 2024.                                                                                                 </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-0301    </td><td>chat  </td><td>openai    </td><td>Legacy Snapshot of gpt-3.5-turbo from March 1st 2023. Will be deprecated on June 13th 2024.                                                                                                    </td></tr>\n",
       "<tr><td>text-davinci-003      </td><td>llm   </td><td>openai    </td><td>Legacy Can do language tasks with better quality and consistency than the curie, babbage, or ada models. Will be deprecated on Jan 4th 2024.                                                   </td></tr>\n",
       "<tr><td>text-davinci-002      </td><td>llm   </td><td>openai    </td><td>Legacy Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning. Will be deprecated on Jan 4th 2024.                                 </td></tr>\n",
       "<tr><td>code-davinci-002      </td><td>llm   </td><td>openai    </td><td>Legacy Optimized for code-completion tasks. Will be deprecated on Jan 4th 2024.                                                                                                                </td></tr>\n",
       "<tr><td>llama-v2-7b-chat      </td><td>chat  </td><td>fireworks </td><td>7b parameter LlamaChat model                                                                                                                                                                   </td></tr>\n",
       "<tr><td>llama-v2-13b-chat     </td><td>chat  </td><td>fireworks </td><td>13b parameter LlamaChat model                                                                                                                                                                  </td></tr>\n",
       "<tr><td>llama-v2-70b-chat     </td><td>chat  </td><td>fireworks </td><td>70b parameter LlamaChat model                                                                                                                                                                  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ModelRegistry(registered_models=[RegisteredModel(name='gpt-3.5-turbo-1106', provider='openai', description='The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Learn more.', params={'model': 'gpt-3.5-turbo-1106'}, type='chat', path=None), RegisteredModel(name='gpt-3.5-turbo', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo'}, type='chat', path=None), RegisteredModel(name='gpt-3.5-turbo-16k', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo-16k'}, type='chat', path=None), RegisteredModel(name='gpt-3.5-turbo-instruct', provider='openai', description='Similar capabilities as text-davinci-003 but compatible with legacy Completions endpoint and not Chat Completions.', params={'model': 'gpt-3.5-turbo-instruct'}, type='llm', path=None), RegisteredModel(name='gpt-3.5-turbo-0613', provider='openai', description='Legacy Snapshot of gpt-3.5-turbo from June 13th 2023. Will be deprecated on June 13, 2024.', params={'model': 'gpt-3.5-turbo-0613'}, type='chat', path=None), RegisteredModel(name='gpt-3.5-turbo-16k-0613', provider='openai', description='Legacy Snapshot of gpt-3.5-16k-turbo from June 13th 2023. Will be deprecated on June 13, 2024.', params={'model': 'gpt-3.5-turbo-16k-0613'}, type='chat', path=None), RegisteredModel(name='gpt-3.5-turbo-0301', provider='openai', description='Legacy Snapshot of gpt-3.5-turbo from March 1st 2023. Will be deprecated on June 13th 2024.', params={'model': 'gpt-3.5-turbo-0301'}, type='chat', path=None), RegisteredModel(name='text-davinci-003', provider='openai', description='Legacy Can do language tasks with better quality and consistency than the curie, babbage, or ada models. Will be deprecated on Jan 4th 2024.', params={'model': 'text-davinci-003'}, type='llm', path=None), RegisteredModel(name='text-davinci-002', provider='openai', description='Legacy Similar capabilities to text-davinci-003 but trained with supervised fine-tuning instead of reinforcement learning. Will be deprecated on Jan 4th 2024.', params={'model': 'text-davinci-002'}, type='llm', path=None), RegisteredModel(name='code-davinci-002', provider='openai', description='Legacy Optimized for code-completion tasks. Will be deprecated on Jan 4th 2024.', params={'model': 'code-davinci-002'}, type='llm', path=None), RegisteredModel(name='llama-v2-7b-chat', provider='fireworks', description='7b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-7b-chat'}, type='chat', path=None), RegisteredModel(name='llama-v2-13b-chat', provider='fireworks', description='13b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-13b-chat'}, type='chat', path=None), RegisteredModel(name='llama-v2-70b-chat', provider='fireworks', description='70b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-70b-chat'}, type='chat', path=None)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974b4f9-575c-4907-97eb-7334ef5f1d8e",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Registry supports indexing by position. This ordering may change as more models get added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64bfc631-1f1e-4cf4-8636-b8be7b46fef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>name       </td><td>gpt-3.5-turbo-1106                                                                                                                                                                             </td></tr>\n",
       "<tr><td>type       </td><td>chat                                                                                                                                                                                           </td></tr>\n",
       "<tr><td>provider   </td><td>openai                                                                                                                                                                                         </td></tr>\n",
       "<tr><td>description</td><td>The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Learn more.</td></tr>\n",
       "<tr><td>model_path </td><td>langchain.chat_models.openai.ChatOpenAI                                                                                                                                                        </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RegisteredModel(name='gpt-3.5-turbo-1106', provider='openai', description='The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Learn more.', params={'model': 'gpt-3.5-turbo-1106'}, type='chat', path=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_model = model_registry[0]\n",
    "registered_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150df8d-11a5-4e83-bc1c-b34119f75783",
   "metadata": {},
   "source": [
    "Can also index by model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "267e746b-f13e-4484-bcbb-ed5dfbacae67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>name       </td><td>gpt-3.5-turbo                          </td></tr>\n",
       "<tr><td>type       </td><td>chat                                   </td></tr>\n",
       "<tr><td>provider   </td><td>openai                                 </td></tr>\n",
       "<tr><td>description</td><td>Currently points to gpt-3.5-turbo-0613.</td></tr>\n",
       "<tr><td>model_path </td><td>langchain.chat_models.openai.ChatOpenAI</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RegisteredModel(name='gpt-3.5-turbo', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo'}, type='chat', path=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry[\"gpt-3.5-turbo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26404672-0832-47be-bc7e-7f74116f6909",
   "metadata": {},
   "source": [
    "## Use the model\n",
    "\n",
    "To use the models, make sure that you have credentials set up. Most models take either an API key as part of the initializer or will use any ENV variables that might be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3604d49e-afbe-48ad-ac10-1e538b1ad376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_registry[\"gpt-3.5-turbo\"]\n",
    "model = registered_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdece532-9843-427a-a10b-4545ed4ec151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I assist you today?')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34801c48-83ed-4ada-b85b-aa3b8cfce31b",
   "metadata": {},
   "source": [
    "## Slicing\n",
    "\n",
    "Slicing notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db40d4da-dc70-4e6d-b7e8-61de1e15ed2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name              </th><th>Type  </th><th>Provider  </th><th>Description                                                                                                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>gpt-3.5-turbo-1106</td><td>chat  </td><td>openai    </td><td>The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Learn more.</td></tr>\n",
       "<tr><td>gpt-3.5-turbo     </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                                        </td></tr>\n",
       "<tr><td>gpt-3.5-turbo-16k </td><td>chat  </td><td>openai    </td><td>Currently points to gpt-3.5-turbo-0613.                                                                                                                                                        </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ModelRegistry(registered_models=[RegisteredModel(name='gpt-3.5-turbo-1106', provider='openai', description='The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Learn more.', params={'model': 'gpt-3.5-turbo-1106'}, type='chat', path=None), RegisteredModel(name='gpt-3.5-turbo', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo'}, type='chat', path=None), RegisteredModel(name='gpt-3.5-turbo-16k', provider='openai', description='Currently points to gpt-3.5-turbo-0613.', params={'model': 'gpt-3.5-turbo-16k'}, type='chat', path=None)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0260af-920f-4512-9273-6f7662369ec5",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "\n",
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9874846a-52f3-4921-b1ed-0858521bb9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name             </th><th>Type  </th><th>Provider  </th><th>Description                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>llama-v2-7b-chat </td><td>chat  </td><td>fireworks </td><td>7b parameter LlamaChat model </td></tr>\n",
       "<tr><td>llama-v2-13b-chat</td><td>chat  </td><td>fireworks </td><td>13b parameter LlamaChat model</td></tr>\n",
       "<tr><td>llama-v2-70b-chat</td><td>chat  </td><td>fireworks </td><td>70b parameter LlamaChat model</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ModelRegistry(registered_models=[RegisteredModel(name='llama-v2-7b-chat', provider='fireworks', description='7b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-7b-chat'}, type='chat', path=None), RegisteredModel(name='llama-v2-13b-chat', provider='fireworks', description='13b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-13b-chat'}, type='chat', path=None), RegisteredModel(name='llama-v2-70b-chat', provider='fireworks', description='70b parameter LlamaChat model', params={'model': 'accounts/fireworks/models/llama-v2-70b-chat'}, type='chat', path=None)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry.filter(provider=\"fireworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59baa6f-c3c1-4e37-919c-f9e70feb9101",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "\n",
    "You can iterate through the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb531591-f46b-4745-ae67-4dfd6217ec5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-0301\n",
      "text-davinci-003\n",
      "text-davinci-002\n",
      "code-davinci-002\n",
      "llama-v2-7b-chat\n",
      "llama-v2-13b-chat\n",
      "llama-v2-70b-chat\n"
     ]
    }
   ],
   "source": [
    "for registered_model in model_registry:\n",
    "    print(registered_model.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
