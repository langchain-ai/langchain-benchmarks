{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bb467d-861d-4b07-a48d-8e5aa177c969",
   "metadata": {},
   "source": [
    "# Q&A over LangChain Docs\n",
    "\n",
    "Let's evaluate your architecture on a Q&A dataset for the LangChain python docs. For more examples of how to test different embeddings, indexing strategies, and architectures, see the [Evaluating RAG Architectures on Benchmark Tasks](./comparing_techniques.ipynb) notebook.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "We will install quite a few prerequisites for this example since we are comparing many techniques and models.\n",
    "\n",
    "We will be using LangSmith to capture the evaluation traces. You can make a free account at [smith.langchain.com](https://smith.langchain.com/). Once you've done so, you can make an API key and set it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44b59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet langchain langsmith langchainhub langchain_benchmarks\n",
    "%pip install --quiet chromadb openai huggingface pandas langchain_experimental sentence_transformers pyarrow anthropic tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae13f6-cd40-41e6-bd02-bd683e91cbff",
   "metadata": {},
   "source": [
    "For this code to work, please configure LangSmith environment variables with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b518cf-99fb-44be-8acb-ee0a8ba62272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls_...\"  # Your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4292ee8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update these with your own API keys\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "# Silence warnings from HuggingFace\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccbf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique run ID for this experiment\n",
    "run_uid = uuid.uuid4().hex[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a666d-8bf5-4bfd-8b20-8b7defdb8cd5",
   "metadata": {},
   "source": [
    "## Review Q&A Tasks\n",
    "\n",
    "The registry provides configurations to test out common architectures on curated datasets. You can review retrieval tasks by filtering on the Type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39159d0-9ea1-414f-a9d8-4a7b22b3d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3644d211-382e-41aa-b282-21b01d28fc35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                   </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LangChain Docs Q&A     </td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Reports</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x2b433fc40>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x2b433fce0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x2b433fd80>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x2b3b59580>}, get_docs=<function load_cached_docs at 0x2b3b59080>), RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x2b43644a0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x2b4364540>, 'hyde': <function _chroma_hyde_retriever_factory at 0x2b43645e0>}, architecture_factories={}, get_docs=<function load_docs at 0x2b4364400>)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry = registry.filter(Type=\"RetrievalTask\")\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671282f8-c455-4390-b018-e53bbd833093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>LangChain Docs Q&A                                                                                                                                         </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID            </td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td></tr>\n",
       "<tr><td>Description           </td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).                                                                                                                                                            </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>basic, parent-doc, hyde                                                                                                                                    </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>conversational-retrieval-qa                                                                                                                                </td></tr>\n",
       "<tr><td>get_docs              </td><td><function load_cached_docs at 0x2b3b59080>                                                                                                                 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x2b433fc40>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x2b433fce0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x2b433fd80>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x2b3b59580>}, get_docs=<function load_cached_docs at 0x2b3b59080>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs = registry[\"LangChain Docs Q&A\"]\n",
    "langchain_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f78bb-7e00-407f-b486-1e5b27f3287a",
   "metadata": {},
   "source": [
    "## Clone the dataset\n",
    "\n",
    "Once you've selected the LangChain Docs Q&A task, clone the dataset to your LangSmith tenant. This step\n",
    "requires that your LANGCHAIN_API_KEY be set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70369f67-deb4-467a-801a-6d38c3d0460d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset LangChain Docs Q&A already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/3f29798f-5939-4643-bd99-008ca66b72ed.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(langchain_docs.dataset_id, dataset_name=langchain_docs.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867f083-1f36-40a9-8e97-74381d47afcd",
   "metadata": {},
   "source": [
    "## Create the index\n",
    "\n",
    "When creating a retrieval Q&A system, the first step is to prepare the retriever. How you construct the index significantly impact your system's performance. Before trying anything too tricky, it's good benchmark a reliable baseline.\n",
    "\n",
    "In this case, our baseline will be to generate a single vector for each raw source document and store them directly in a vector store.\n",
    "\n",
    "Below, fetch the source docs from the cache in GCS. This cache was formed using an [ingestion script](https://github.com/langchain-ai/langchain-benchmarks/blob/30aa706d9cefbaebb219f1763c04fafab6d0ee78/langchain_benchmarks/rag/tasks/langchain_docs/_ingest_docs.py#L1) that scraped the LangChain documentation. To save time and to ensure that the dataset answers are still correct, we will use these source docs for all benchmark approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3afe1e54-9354-4bbc-9fd8-0f9d769fc43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(page_content=\"LangChain cookbook | 🦜️🔗 Langchain\\n\\n[Skip to main content](#docusaurus_skip...\n"
     ]
    }
   ],
   "source": [
    "docs = list(langchain_docs.get_docs())\n",
    "print(repr(docs[0])[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6830a-dd2f-4532-95e7-81353d18dcd2",
   "metadata": {},
   "source": [
    "Now we will populate our vectorstore. We will use LangChain's indexing API to cache embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4980f8d4-d4e6-4b70-a04d-a7d9333545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-base\",\n",
    "    # model_kwargs={\"device\": 0},  # Comment out to use CPU\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=f\"lcbm-b-huggingface-gte-base\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chromadb\",\n",
    ")\n",
    "\n",
    "vectorstore.add_documents(docs)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bac7ea-ab28-4b08-9d06-56ac869085dd",
   "metadata": {},
   "source": [
    "## Define the response generator\n",
    "\n",
    "Halfway done with our RAG system. We've made the **R**etriever. Now time for the response **G**enerator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41e64350-63a7-4e7d-8e03-7dc459c444cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "\n",
    "# After the retriever fetches documents, this\n",
    "# function formats them in a string to present for the LLM\n",
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = (\n",
    "            f\"<document index='{i}'>\\n\"\n",
    "            f\"<source>{doc.metadata.get('source')}</source>\\n\"\n",
    "            f\"<doc_content>{doc.page_content}</doc_content>\\n\"\n",
    "            \"</document>\"\n",
    "        )\n",
    "        formatted_docs.append(doc_string)\n",
    "    formatted_str = \"\\n\".join(formatted_docs)\n",
    "    return f\"<documents>\\n{formatted_str}\\n</documents>\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an AI assistant answering questions about LangChain.\"\n",
    "            \"\\n{context}\\n\"\n",
    "            \"Respond solely based on the document content.\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatAnthropic(model=\"claude-2.1\", temperature=1)\n",
    "\n",
    "response_generator = (prompt | llm | StrOutputParser()).with_config(\n",
    "    run_name=\"GenerateResponse\",\n",
    ")\n",
    "\n",
    "# This is the final response chain.\n",
    "# It fetches the \"question\" key from the input dict,\n",
    "# passes it to the retriever, then formats as a string.\n",
    "\n",
    "chain = (\n",
    "    RunnableAssign(\n",
    "        {\n",
    "            \"context\": (itemgetter(\"question\") | retriever | format_docs).with_config(\n",
    "                run_name=\"FormatDocs\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    # The \"RunnableAssign\" above returns a dict with keys\n",
    "    # question (from the original input) and\n",
    "    # context: the string-formatted docs.\n",
    "    # This is passed to the response_generator above\n",
    "    | response_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10a1fca9-d356-4cff-93a9-c4f63944e57d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The LangChain Expression Language (LCEL) is a declarative way to easily compose chains of different components like prompts, models, parsers, etc. \\n\\nSome key things it provides:\\n\\n- Streaming support - Ability to get incremental outputs from chains rather than waiting for full completion. Useful for long-running chains.\\n\\n- Async support - Chains can be called synchronously (like in a notebook) or asynchronously (like in production). Same code works for both.\\n\\n- Optimized parallel execution - Steps that can run in parallel (like multiple retrievals) are automatically parallelized to minimize latency.\\n\\n- Retries and fallbacks - Help make chains more robust to failure.\\n\\n- Access to intermediate results - Useful for debugging or showing work-in-progress.\\n\\n- Input and output validation via schemas - Enables catching issues early.\\n\\n- Tracing - Automatic structured logging of all chain steps for observability.\\n\\n- Seamless deployment - LCEL chains can be easily deployed with LangServe.\\n\\nThe key idea is it makes it very easy to take a prototype LLM application made with components like prompts and models and turn it into a robust, scalable production application without changing any code.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What's expression language?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821e4b0-8e67-418a-840c-470fcde42df0",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Let's evaluate your RAG architecture on the dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "513042fe-2878-44f8-ae84-05b9d521c1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aab7514e-a6ef-4c21-b90f-d9cbefcf5af1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'claude-2 qa-chain simple-index 1bdbe5' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/3fe31959-95e8-4413-aa09-620bd49bd0d3?eval=true\n",
      "\n",
      "View all tests for Dataset LangChain Docs Q&A at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/3f29798f-5939-4643-bd99-008ca66b72ed\n",
      "[------------------------------------------------->] 86/86\n",
      " Eval quantiles:\n",
      "                                0.25        0.5       0.75       mean  \\\n",
      "embedding_cosine_distance   0.088025   0.115760   0.159969   0.129161   \n",
      "score_string:accuracy       0.500000   0.700000   1.000000   0.645349   \n",
      "faithfulness                0.700000   1.000000   1.000000   0.812791   \n",
      "execution_time             27.098772  27.098772  27.098772  27.098772   \n",
      "\n",
      "                                mode  \n",
      "embedding_cosine_distance   0.048622  \n",
      "score_string:accuracy       0.700000  \n",
      "faithfulness                1.000000  \n",
      "execution_time             27.098772  \n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "RAG_EVALUATION = get_eval_config()\n",
    "\n",
    "test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"claude-2 qa-chain simple-index {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d85a82-5ba7-479f-94e2-c9e3c7cb90cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230bf290-0422-4774-b82f-f78eb2e51918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
