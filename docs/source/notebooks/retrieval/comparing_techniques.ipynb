{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2af633b-4d0f-4b80-b090-2d6429f22e90",
   "metadata": {},
   "source": [
    "# Evaluating RAG Architectures on Benchmark Tasks\n",
    "\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "If you ever wanted to compare different approaches to Q&A over docs, you'll find this notebook helpful to get started evaluating different configurations and common RAG architectures on benchmark tasks. The goal is to make it easy for you to experiment with different techniques, understand their tradeoffs, and make informed decisions for your specific use case.\n",
    "\n",
    "#### What is RAG?\n",
    "\n",
    "LLMs have a knowledge cutoff. For them to accurately respond to user queries, they need access to relevant information. Retrieval Augmented Generation (RAG) (aka \"give an LLM a search engine\") is a common design pattern to address this. The key components are:\n",
    "\n",
    "- Retriever: fetches information from a knowledge base, which can be a vector search engine, a database, or any search engine.\n",
    "- Generator: synthesizes responses using a blend of learned knowledge and the retrieved information.\n",
    "\n",
    "The overall quality of the system depends on both components.\n",
    "\n",
    "\n",
    "#### Benchmark Tasks and Datasets (As of 2023/11/21)\n",
    "\n",
    "The following datasets are currently available:\n",
    "\n",
    "- LangChain Docs Q&A - technical questions based on the LangChain python documentation\n",
    "- Semi-structured Earnings - financial questions and answers on financial PDFs containing tables and graphs\n",
    "\n",
    "Each task comes with a labeled dataset of questions and answers. They also provide configurable factory functions for easy customization of chunking and indexing for the relevant source documents.\n",
    "\n",
    "And with that, let's get started!\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "We will install quite a few prerequisites for this example since we are comparing many techniques and models.\n",
    "\n",
    "We will be using LangSmith to capture the evaluation traces. You can make a free account at [smith.langchain.com](https://smith.langchain.com/). Once you've done so, you can make an API key and set it below.\n",
    "\n",
    "We are comparing many methods throughout this notebook, so the list of dependencies we will install is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U --quiet langchain langsmith langchainhub langchain_benchmarks\n",
    "%pip install --quiet chromadb openai huggingface pandas langchain_experimental sentence_transformers pyarrow anthropic tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b518cf-99fb-44be-8acb-ee0a8ba62272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"sk-...\"  # Your API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Your OpenAI API key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"  # Your Anthropic API key\n",
    "# Silence warnings from HuggingFace\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique run ID for these experiments\n",
    "run_uid = uuid.uuid4().hex[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a666d-8bf5-4bfd-8b20-8b7defdb8cd5",
   "metadata": {},
   "source": [
    "## Review Q&A tasks\n",
    "\n",
    "The registry provides configurations to test out common architectures on curated datasets.\n",
    "Below is a list of the available tasks at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39159d0-9ea1-414f-a9d8-4a7b22b3d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3644d211-382e-41aa-b282-21b01d28fc35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                   </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LangChain Docs Q&A     </td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Reports</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x12aae2840>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x12aae28e0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x12aae2980>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x12a1be020>}, get_docs=<function load_cached_docs at 0x12a1bdb20>), RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x12aae3060>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x12aae3100>, 'hyde': <function _chroma_hyde_retriever_factory at 0x12aae31a0>}, architecture_factories={}, get_docs=<function load_docs at 0x12aae2fc0>)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.filter(Type=\"RetrievalTask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671282f8-c455-4390-b018-e53bbd833093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_docs = registry[\"LangChain Docs Q&A\"]\n",
    "langchain_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70369f67-deb4-467a-801a-6d38c3d0460d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clone_public_dataset(langchain_docs.dataset_id, dataset_name=langchain_docs.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02011398-1a6f-42c1-b586-9d01c78e3ee4",
   "metadata": {},
   "source": [
    "## Basic Vector Retrieval\n",
    "\n",
    "For our first example, we will generate a single embedding for each document in the dataset,\n",
    "without chunking or indexing, and then provide that retriever to an LLM for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58247f5-b9bd-4cc5-9632-78bc21bb10b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-base\",\n",
    "    model_kwargs={\"device\": 0},  # Comment out to use CPU\n",
    ")\n",
    "\n",
    "retriever_factory = langchain_docs.retriever_factories[\"basic\"]\n",
    "# Indexes the documents with the specified embeddings\n",
    "# Note that this does not apply any chunking to the docs,\n",
    "# which means the documents can be of arbitrary length\n",
    "retriever = retriever_factory(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2e139-2653-4f7b-944b-91ef52f43d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Factory for creating a conversational retrieval QA chain\n",
    "\n",
    "chain_factory = langchain_docs.architecture_factories[\"conversational-retrieval-qa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9be718-64f0-4706-9527-240a1cdb3ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "# Example\n",
    "llm = ChatAnthropic(model=\"claude-2\", temperature=1)\n",
    "\n",
    "chain_factory(retriever, llm=llm).invoke({\"question\": \"what's lcel?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513042fe-2878-44f8-ae84-05b9d521c1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7514e-a6ef-4c21-b90f-d9cbefcf5af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client()\n",
    "RAG_EVALUATION = get_eval_config()\n",
    "\n",
    "test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, retriever, llm=llm),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"claude-2 qa-chain simple-index {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"llm\": \"claude-2\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86578d5-be5c-4bcd-9dcb-35280eeed3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee992f87-4137-49b1-a1f1-0cc7be0e32d8",
   "metadata": {},
   "source": [
    "# Comparing with other indexing strategies\n",
    "\n",
    "The index used above retrieves the raw documents based on a single vector per document. It doesn't perform any additional chunking. You can try changing the chunking parameters when generating the index.\n",
    "\n",
    "## Customizing Chunking\n",
    "\n",
    "The simplest change you can make to the index is configure how you split the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72030d4-c201-44b8-85cd-903afa313f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def transform_docs(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n",
    "    yield from splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# Used for the cache\n",
    "transformation_name = \"recursive-text-cs4k-ol200\"\n",
    "\n",
    "retriever_factory = langchain_docs.retriever_factories[\"basic\"]\n",
    "\n",
    "chunked_retriever = retriever_factory(\n",
    "    embeddings,\n",
    "    transform_docs=transform_docs,\n",
    "    transformation_name=transformation_name,\n",
    "    search_kwargs={\"k\": 4},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f12f9-1ba6-4bf7-a850-4073fb0994f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunked_results = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, chunked_retriever, llm=llm),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"claude-2 qa-chain chunked {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "        \"chunk_size\": 4000,\n",
    "        \"chunk_overlap\": 200,\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"llm\": \"claude-2\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d825f1-9a91-429d-bf3e-a9b9c2785a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunked_results.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a62ec-9a9c-4d7a-ab90-97020d855ee7",
   "metadata": {},
   "source": [
    "## Parent Document Retriever\n",
    "\n",
    "This indexing technique chunks documents and generates 1 vector per chunk.\n",
    "At retrieval time, the K \"most similar\" chunks are fetched, then the full parent documents are returned for the LLM to reason over.\n",
    "\n",
    "This ensures the chunk is surfaced in its full natural context. It also can potentially improve the initial retrieval quality since the similarity scores are scoped to individual chunks.\n",
    "\n",
    "Let's see if this technique is effective in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398f5e3-b7fe-4693-bcc0-c6c6f75c8234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever_factory = langchain_docs.retriever_factories[\"parent-doc\"]\n",
    "\n",
    "# Indexes the documents with the specified embeddings\n",
    "parent_doc_retriever = retriever_factory(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f4b5d-143a-44ce-95f4-d0b5782ada74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parent_doc_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, parent_doc_retriever, llm=llm),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"claude-2 qa-chain parent-doc {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"parent-doc\",\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"llm\": \"claude-2\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef0410-47ec-4830-9b75-621eb85240ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parent_doc_test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b27dd0-f0df-4551-a972-1a6c0df5ffb9",
   "metadata": {},
   "source": [
    "## HyDE\n",
    "\n",
    "HyDE (Hypothetical document embeddings) refers to the technique of using an LLM\n",
    "to generate example queries that my be used to retrieve a doc. By doing so, the resulting embeddings are automatically \"more aligned\" with the embeddings generated from the query. This comes with an additional indexing cost, since each document requires an additoinal call to an LLM while indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92d2c2-f410-43cc-9c9f-abc22ef48353",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_factory = langchain_docs.retriever_factories[\"hyde\"]\n",
    "\n",
    "retriever = retriever_factory(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179cf29-2d75-4a04-bbb5-b8f22028fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, retriever=retriever, llm=llm),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    "    project_name=f\"claude-2 qa-chain HyDE {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"HyDE\",\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"llm\": \"claude-2\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a04f21-0308-4b00-a6f1-694d98ba7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8af309-d0c4-4562-a5f0-30ca9f9fd861",
   "metadata": {},
   "source": [
    "# Comparing Embeddings\n",
    "\n",
    "We've been using off-the-shelf GTE-Base embeddings so far to retrieve the docs, but\n",
    "you may get better results with other embeddings. You could even try fine-tuning embedddings on your own documentation and evaluating here.\n",
    "\n",
    "Let's compare our results so far to OpenAI's embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b2395-c07e-4eae-bb21-afdda3961cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5edab-9a3a-4864-b69f-69bc1c9e7816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_retriever = langchain_docs.retriever_factories[\"basic\"](openai_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757c411-aaa5-42ad-824c-7c0b5b942e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_embeddings_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, openai_retriever),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"claude-2 qa-chain oai-emb basic {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "        \"embedding_model\": \"openai/text-embedding-ada-002\",\n",
    "        \"llm\": \"claude-2\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae7cbe-a8eb-4b40-aeae-f9c7f4bf335f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_embeddings_test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef164b9-7124-4907-b2b4-0595bf3b3441",
   "metadata": {},
   "source": [
    "## Comparing Models\n",
    "\n",
    "We used Anthropic's Claude-2 model in our previous tests, but lets try with some other models.\n",
    "\n",
    "You can swap in any LangChain LLM within the response generator below.\n",
    "We'll try a long-context llama 2 model first (using Ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c86c7-9754-4527-a1a9-a89beba437b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "# A llama2-based model with 128k context\n",
    "# (in theory) In practice, we will see how well\n",
    "# it actually leverages that context.\n",
    "ollama = ChatOllama(model=\"yarn-llama2:7b-128k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7dff86-2b93-490a-81ab-72e757e8f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll go back to the GTE embeddings for now\n",
    "\n",
    "retriever_factory = langchain_docs.retriever_factories[\"basic\"]\n",
    "retriever = retriever_factory(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa47085-e383-4cc5-9018-5491700c6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, llm=ollama, retriever=retriever),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"yarn-llama2:7b-128k qa-chain basic {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"llm\": \"ollama/yarn-llama2:7b-128k\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edcf42-a405-400b-882e-04de2559359c",
   "metadata": {},
   "source": [
    "## Changing the prompt in the response generator\n",
    "\n",
    "The default prompt was tested primariily on OpenAI's gpt-3.5 model. When switching models, you may get better results if you modify the prompt. Let's try a simple one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3b36f-68aa-4005-9bb2-de228491ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64caac6f-888d-432c-9329-5c4b97ad859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"wfh/rag-simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e6762-1e50-4eef-833a-a4a2bf8883ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = prompt | ChatAnthropic(model=\"claude-2\", temperature=1) | StrOutputParser()\n",
    "new_chain = chain_factory(response_generator=generator, retriever=openai_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96886de0-a653-4875-a68f-5a11efcb200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_simple_prompt_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(\n",
    "        chain_factory, response_generator=generator, retriever=retriever, llm=llm\n",
    "    ),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"claude-2 qa-chain basic rag-simple {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"prompt\": \"wfh/rag-simple\",\n",
    "        \"llm\": \"claude-2\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffaf28-902e-4466-b3b9-25441d45585d",
   "metadata": {},
   "source": [
    "## Testing Agents\n",
    "\n",
    "Agents use an LLM to decide actions and generate responses. There are two obvious ways they could potentially succeed where the approaches above fail:\n",
    "- The above chains do not \"rephrase\" the user query. It could be that the rephrased question will result in more relevant documents.\n",
    "- The above chains must respond based on a single retrieval step. Agents can iteratively query the retriever or subdivide the query into different parts to synthesize at the end. Our dataset has a number of questions that require information from different documents - if the\n",
    "\n",
    "Let's evaluate to see whether the \"plausible\" statements above are worth the tradeoffs. We will use the basic retriever as a tool for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c19c4-f8d6-41b3-9389-e89abd4b5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "# This is used to tell the model how to best use the retriever.\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query, callbacks=None):\n",
    "    \"\"\"Search the LangChain docs with the retriever.\"\"\"\n",
    "    return retriever.get_relevant_documents(query, callbacks=callbacks)\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "assistant_system_message = \"\"\"You are a helpful assistant tasked with answering technical questions about LangChain. \\\n",
    "Use tools (only if necessary) to best answer the users questions. Do not make up information if you cannot find the answer using your tools.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", assistant_system_message),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]):\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_functions(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "class AgentInput(BaseModel):\n",
    "    input: str\n",
    "    chat_history: List[Tuple[str, str]] = Field(..., extra={\"widget\": {\"type\": \"chat\"}})\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False).with_types(\n",
    "    input_type=AgentInput\n",
    ")\n",
    "\n",
    "\n",
    "class ChainInput(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "def mapper(input: dict):\n",
    "    return {\"input\": input[\"question\"], \"chat_history\": []}\n",
    "\n",
    "\n",
    "agent_executor = (mapper | agent_executor | (lambda x: x[\"output\"])).with_types(\n",
    "    input_type=ChainInput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c09c2-a983-450b-a531-c6871a9b27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_functions_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=agent_executor,\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"oai-functions basic rag-simple {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"llm\": \"gpt-4-1106-preview\",\n",
    "        \"architecture\": \"oai-functions-agent\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ffd740-fde1-479d-b84c-7dd8f65716a6",
   "metadata": {},
   "source": [
    "## Assistant\n",
    "\n",
    "OpenAI provides a hosted agent service through their Assistants API. \n",
    "\n",
    "You can connect your LangChain retriever to an OpenAI's Assistant API and evaluate its performance. Let's test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4d42b-e34a-4980-97eb-9b2c78a24089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools import tool\n",
    "from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query, callbacks=None) -> str:\n",
    "    \"\"\"Search the LangChain docs with the retriever.\"\"\"\n",
    "    docs = retriever.get_relevant_documents(query, callbacks=callbacks)\n",
    "    return json.dumps([doc.dict() for doc in docs])\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "\n",
    "agent = OpenAIAssistantRunnable.create_assistant(\n",
    "    name=\"langchain docs assistant\",\n",
    "    instructions=\"You are a helpful assistant tasked with answering technical questions about LangChain.\",\n",
    "    tools=tools,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    as_agent=True,\n",
    ")\n",
    "\n",
    "\n",
    "assistant_exector = (\n",
    "    (lambda x: {\"content\": x[\"question\"]})\n",
    "    | AgentExecutor(agent=agent, tools=tools)\n",
    "    | (lambda x: x[\"output\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6baea1-ac90-43aa-a21c-98aa5ca23732",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=assistant_exector,\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    project_name=f\"oai-assistant basic rag-simple {run_uid}\",\n",
    "    project_metadata={\n",
    "        \"index_method\": \"basic\",\n",
    "        \"embedding_model\": \"thenlper/gte-base\",\n",
    "        \"llm\": \"gpt-4-1106-preview\",\n",
    "        \"architecture\": \"oai-assistant\",\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac5fad-0a74-4403-a917-7145be6d7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04b3e4-b5df-4075-9089-8aa10ef63348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
