{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec8815f-79f5-49b1-a29d-48acc7795e57",
   "metadata": {},
   "source": [
    "# Multi-modal eval\n",
    "\n",
    "GPT-4V w/ (1) multi-vector retriever and (2) multi-modal embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df8210-6b5a-40bc-a3b8-6682a0148c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain openai chromadb pypdfium2 open_clip pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ddce4b-f94e-42d8-9648-d627508ae476",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c96ea3-ac70-44c7-9751-b6324e98e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "\n",
    "import pypdfium2 as pdfium\n",
    "from IPython.display import HTML, display\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1e38a-e24c-43aa-86b0-491432a4caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File\n",
    "fpath = \"./\"\n",
    "fname = \"DDOG_Q3_earnings_deck.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89327e0b-2c07-40ec-90ef-b3e151184dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(fname):\n",
    "    \"\"\"\n",
    "    Get PIL images from PDF pages\n",
    "    \"\"\"\n",
    "    pdf = pdfium.PdfDocument(fname)\n",
    "    n_pages = len(pdf)\n",
    "    pil_images = []\n",
    "    for page_number in range(n_pages):\n",
    "        page = pdf.get_page(page_number)\n",
    "        bitmap = page.render(scale=1, rotation=0, crop=(0, 0, 0, 0))\n",
    "        pil_image = bitmap.to_pil()\n",
    "        pil_images.append(pil_image)\n",
    "        pil_image.save(fpath + \"img_%s.jpg\" % str(page_number + 1), format=\"JPEG\")\n",
    "    return pil_images\n",
    "\n",
    "pil_images = get_images(fpath, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c02337-ce0f-4d02-8db5-78219cda3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Base64 encoded strings\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    img_str = resize_base64_image(img_str, size=(1280, 720))\n",
    "    return img_str\n",
    "\n",
    "\n",
    "images_base_64 = [convert_to_base64(i) for i in pil_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07145e9b-2dca-4aae-971c-4b02cb8dd2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "i = 10\n",
    "plt_img_base64(images_base_64[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2046a4-c587-4ac1-92b4-25f933a53028",
   "metadata": {},
   "source": [
    "## Summarize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0731d-71ae-4071-bd05-13bed2df7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(img_base64_list):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    img_base64_list: Base64 encoded images\n",
    "    \"\"\"\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for i, base64_image in enumerate(img_base64_list):\n",
    "        image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "image_summaries = generate_img_summaries(images_base_64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b95160-f064-4239-bc89-791d85e93117",
   "metadata": {},
   "source": [
    "## Make Retriever\n",
    "\n",
    "### MV Retriever\n",
    "\n",
    "Add raw docs and doc summaries to [Multi Vector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary): \n",
    "\n",
    "* Store the raw images in the `docstore`.\n",
    "* Store the image summaries in the `vectorstore` for semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83ae31-c3f1-406f-b88f-94eb60ddbe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(vectorstore, image_summaries, images):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi-modal-rag-deck\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    image_summaries,\n",
    "    images_base_64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11ea16-fbc7-4eef-b618-4d137c01579d",
   "metadata": {},
   "source": [
    "### Multi modal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63820b6-cc12-44b6-80c3-0c765f089af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chroma\n",
    "vectorstore_mmembd = Chroma(\n",
    "    collection_name=\"multi-modal-rag-deck-mmembd\",\n",
    "    embedding_function=OpenCLIPEmbeddings(),\n",
    ")\n",
    "\n",
    "# Get image URIs with .jpg extension only\n",
    "image_uris = sorted(\n",
    "    [\n",
    "        os.path.join(fpath, image_name)\n",
    "        for image_name in os.listdir(fpath)\n",
    "        if image_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add images\n",
    "vectorstore_mmembd.add_images(uris=image_uris)\n",
    "\n",
    "# Make retriever\n",
    "retriever_mmembd = vectorstore_mmembd.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c44a67-8639-4a30-b659-5957df89c5d6",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032d4e0-63d5-4e7f-bafc-e0550b5c2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            # Note: OAI throws bad request error on retrieval when image size is\n",
    "            # too large. The behavior is not obvious (images << 20MB threshold)\n",
    "            # and needs further investigation. For now, empirically tuning the\n",
    "            # image size does work, but a systematic solution is needed.\n",
    "            doc = resize_base64_image(doc, size=(1280, 720))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict, num_images=2):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        # Restrict prompt to num_images\n",
    "        for image in data_dict[\"context\"][\"images\"][:num_images]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an analyst tasked with answering questions about visual content.\\n\"\n",
    "            \"You will be give a set of image(s) from a slide deck / presentaqtion.\\n\"\n",
    "            \"Use this information to answer the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "# chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "chain_multimodal_rag_mmembd = multi_modal_rag_chain(retriever_mmembd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2af1a-0b36-44be-9b3a-7366dd05fd01",
   "metadata": {},
   "source": [
    "## Test retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77971a48-365f-4c62-9fa5-838f732beb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever_mmembd.get_relevant_documents(\n",
    "    \"What is the projected TAM over time for observability?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bd122-ef19-4b8c-8761-25bcdd97e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_img_base64(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6b82b-bdef-4e62-b62d-1b2b62775b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever_multi_vector_img.get_relevant_documents(\n",
    "    \"What is the projected TAM over time for observability?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b609b-030f-41c0-847a-54d1ce378be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_img_base64(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d8178-0de6-42da-b26b-1eceadc0b8aa",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5e90c-9e2d-4e35-ac93-73ec5e0b42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag_mmembd.invoke(\n",
    "    \"What is the projected TAM over time for observability?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e54e4-4108-48de-a893-77111a42749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag.invoke(\"What is the projected TAM over time for observability?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20374beb-3ac5-4989-ada1-27d6a7fb472d",
   "metadata": {},
   "source": [
    "See [trace](https://smith.langchain.com/public/21c61c87-2bdd-4c38-9967-b9df9f5549fb/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d5726-3d1d-4ea3-8c6b-665992f0a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag.invoke(\"What are the components of the Datadog platform?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbab5c7-e9eb-4c60-aed8-15b9c1e23e1b",
   "metadata": {},
   "source": [
    "See [trace](https://smith.langchain.com/public/dbe9e2b5-d21c-41a2-b037-60573035dbe9/r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e19897-501c-4336-a67f-8200f931906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag.invoke(\"How many total customers does Datadog have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc9e2f-60aa-4a37-84be-56cff5282983",
   "metadata": {},
   "source": [
    "See [trace](https://smith.langchain.com/public/fbaf39df-abd4-4258-bd61-a1ced4073774/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5f379-317c-4a2e-9190-61f6dfbbc77d",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f62cb-3dcd-4403-aa29-6f7497836439",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Replace with public dataset\n",
    "\n",
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(\n",
    "    \"./multi_modal_presentations.csv\"\n",
    ")\n",
    "\n",
    "# Dataset\n",
    "client = Client()\n",
    "dataset_name = \"Multi-Modal-Eval\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "# Populate dataset\n",
    "for _, row in df.iterrows():\n",
    "    # Get Q, A\n",
    "    q = row[\"Question\"]\n",
    "    a = row[\"Answer\"]\n",
    "\n",
    "    # Use the values in your function\n",
    "    client.create_example(\n",
    "        inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b5f1f-cdde-4a93-b3c5-4a66ea4f6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.client import Client\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")\n",
    "\n",
    "\n",
    "def run_eval(chain, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    test_run = client.run_on_dataset(\n",
    "        ### TODO: Replace with public dataset\n",
    "        dataset_name=\"Multi-Modal-Eval\",\n",
    "        llm_or_chain_factory=lambda: (lambda x: x[\"question\"]) | chain,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=eval_run_name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"multi_modal_mvretriever_gpt4v\": chain_multimodal_rag,\n",
    "    \"multi_modal_mmembd_gpt4v\": chain_multimodal_rag_mmembd,\n",
    "}\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "for project_name, chain in chain_map.items():\n",
    "    run_eval(chain, project_name + \"_\" + run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
