{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa3470d-9448-4792-9f65-6978fc58cf84",
   "metadata": {},
   "source": [
    "# Multi-modal eval\n",
    "\n",
    "Top-K RAG (without multi-modal reasoning)\n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47220461-d4e9-4f1d-9c57-672ca947ca0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -U langchain langsmith langchain_benchmarks\n",
    "# %pip install --quiet chromadb openai pypdf pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196de967-6de6-40da-aa75-e836923ab5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "env_vars = [\"LANGCHAIN_API_KEY\", \"OPENAI_API_KEY\"]\n",
    "for var in env_vars:\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass.getpass(prompt=f\"Enter your {var}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f4613-46c3-4719-9f75-678e3b4932ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"sk-...\"  # Your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da8e11-6288-4131-bd60-d5aa86928acc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20af42d4-9dfb-45ef-9387-432748ccfc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff97905-14a6-413c-99be-58b7a9c8d4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                   </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LangChain Docs Q&A     </td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Multi-modal slide decks</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d\" target=\"_blank\" rel=\"noopener\">40afc8e7-9d7e-44ed-8971-2cae1eb59731</a></td><td>This public dataset is a work-in-progress and will be extended over time.\n",
       "        \n",
       "Questions and answers based on slide decks containing visual tables and charts.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Reports</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x173d55700>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x173d55790>, 'hyde': <function _chroma_hyde_retriever_factory at 0x173d55820>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x172f1c4c0>}, get_docs=<function load_cached_docs at 0x172f1c310>), RetrievalTask(name='Multi-modal slide decks', dataset_id='https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d', description=\"This public dataset is a work-in-progress and will be extended over time.\\n        \\nQuestions and answers based on slide decks containing visual tables and charts.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={}, architecture_factories={}, get_docs={}), RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x173d55ca0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x173d55d30>, 'hyde': <function _chroma_hyde_retriever_factory at 0x173d55dc0>}, architecture_factories={}, get_docs=<function load_docs at 0x173d55c10>)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry = registry.filter(Type=\"RetrievalTask\")\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219a4141-4a5f-48e4-ae05-5a824e2193fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>Multi-modal slide decks                                                                                                                                    </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID            </td><td><a href=\"https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d\" target=\"_blank\" rel=\"noopener\">40afc8e7-9d7e-44ed-8971-2cae1eb59731</a></td></tr>\n",
       "<tr><td>Description           </td><td>This public dataset is a work-in-progress and will be extended over time.\n",
       "        \n",
       "Questions and answers based on slide decks containing visual tables and charts.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).                                                                                                                                                            </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>                                                                                                                                                           </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>                                                                                                                                                           </td></tr>\n",
       "<tr><td>get_docs              </td><td>{}                                                                                                                                                         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='Multi-modal slide decks', dataset_id='https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d', description=\"This public dataset is a work-in-progress and will be extended over time.\\n        \\nQuestions and answers based on slide decks containing visual tables and charts.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={}, architecture_factories={}, get_docs={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = registry[\"Multi-modal slide decks\"]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2caa086-9549-4c74-bba9-ba80d5a7b218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05864968bad64a0f86c789c049d40a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching examples. Creating dataset...\n",
      "New dataset created you can access it at https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74.\n",
      "Done creating dataset.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(task.dataset_id, dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ce6afb-2317-4bc1-9faf-4f828095ad91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_file_names' from 'langchain_benchmarks.rag.tasks.multi_modal_slide_decks' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_benchmarks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_modal_slide_decks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_file_names\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# If you want to completely customize the document processing, you can use the files directly\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(get_file_names())\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_file_names' from 'langchain_benchmarks.rag.tasks.multi_modal_slide_decks' (unknown location)"
     ]
    }
   ],
   "source": [
    "from langchain_benchmarks.rag.tasks.multi_modal_slide_decks import get_file_names\n",
    "\n",
    "# If you want to completely customize the document processing, you can use the files directly\n",
    "file_names = list(get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96370f4-1d4f-4867-990d-7d8153142e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                   </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LangChain Docs Q&A     </td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Reports</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Multi-modal Slide Decks</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d\" target=\"_blank\" rel=\"noopener\">40afc8e7-9d7e-44ed-8971-2cae1eb59731</a></td><td>The Multi-modal Slide Decks dataset is a collection of question-answer pairs\n",
       "pertaining to a series of slide decks formatted as PDFs. The questions are\n",
       "designed to be answerable best by models that can reason over both text and\n",
       "images.              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", get_docs=<function load_cached_docs at 0x1212aafc0>, retriever_factories={'basic': <function _chroma_retriever_factory at 0x13082d800>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x13082d8a0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x13082d940>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x1212ab4c0>}), RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", get_docs=<function load_docs at 0x13082df80>, retriever_factories={'basic': <function _chroma_retriever_factory at 0x13082e020>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x13082e0c0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x13082e160>}, architecture_factories={}), RetrievalTask(name='Multi-modal Slide Decks', dataset_id='https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d', description='The Multi-modal Slide Decks dataset is a collection of question-answer pairs\\npertaining to a series of slide decks formatted as PDFs. The questions are\\ndesigned to be answerable best by models that can reason over both text and\\nimages.\\n', get_docs=None, retriever_factories={}, architecture_factories={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "\n",
    "registry = registry.filter(Type=\"RetrievalTask\")\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae949f3b-4fe0-4b44-8f61-187ced7e542f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>Multi-modal Slide Decks                                                                                                                                    </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID            </td><td><a href=\"https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d\" target=\"_blank\" rel=\"noopener\">40afc8e7-9d7e-44ed-8971-2cae1eb59731</a></td></tr>\n",
       "<tr><td>Description           </td><td>The Multi-modal Slide Decks dataset is a collection of question-answer pairs\n",
       "pertaining to a series of slide decks formatted as PDFs. The questions are\n",
       "designed to be answerable best by models that can reason over both text and\n",
       "images.                                                                                                                                                            </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>                                                                                                                                                           </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>                                                                                                                                                           </td></tr>\n",
       "<tr><td>get_docs              </td><td>                                                                                                                                                           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='Multi-modal Slide Decks', dataset_id='https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d', description='The Multi-modal Slide Decks dataset is a collection of question-answer pairs\\npertaining to a series of slide decks formatted as PDFs. The questions are\\ndesigned to be answerable best by models that can reason over both text and\\nimages.\\n', get_docs=None, retriever_factories={}, architecture_factories={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = registry[\"Multi-modal Slide Decks\"]\n",
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c0ef6-87a1-43b0-aaae-3feddf0c070f",
   "metadata": {},
   "source": [
    "## Clone the dataset of Q&A pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb6cca7-2e95-47a9-a264-104fc7908ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Multi-modal Slide Decks already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/7392a130-f6c7-4e60-9059-55fed96dd7a0.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(task.dataset_id, dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a4cdb-6c08-4c01-81ce-16ab83a7fdff",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "We will fetch the files from the remote cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce85810-98a7-406e-b44e-ce860ac35986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 98 text elements in DDOG_Q3_earnings_deck.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_benchmarks.rag.tasks.multi_modal_slide_decks import get_file_names\n",
    "\n",
    "# Fetch the PDFs from the remote cache\n",
    "file_names = list(get_file_names())\n",
    "\n",
    "\n",
    "def load_and_split(file):\n",
    "    \"\"\"\n",
    "    Load and split PDF files\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFLoader(str(file))\n",
    "    pdf_pages = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=100, chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    # Get chunks\n",
    "    docs = text_splitter.split_documents(pdf_pages)\n",
    "    texts = [d.page_content for d in docs]\n",
    "    print(f\"There are {len(texts)} text elements in {file.name}\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "texts = []\n",
    "for fi in file_names:\n",
    "    texts.extend(load_and_split(fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01925d-b7d1-47a1-bd90-805178d3c4a9",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb31f71-45fb-4b12-bc1c-31981de334bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore_baseline = Chroma.from_texts(\n",
    "    texts=texts, collection_name=\"baseline-multi-modal\", embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_baseline = vectorstore_baseline.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dcbb01-f480-456d-b972-c732eb26c393",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea233664-e527-42f1-a820-0c2271e16c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | (lambda x: \"\\n\\n\".join([i.page_content for i in x])),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "chain = rag_chain(retriever_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "479ce09d-642e-4b3b-9e4e-e9c2b7f0e9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_run_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m test_runs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m project_name, chain \u001b[38;5;129;01min\u001b[39;00m chain_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     21\u001b[0m     test_runs[project_name] \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mrun_on_dataset(\n\u001b[1;32m     22\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     23\u001b[0m         llm_or_chain_factory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: (\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m|\u001b[39m chain,\n\u001b[1;32m     24\u001b[0m         evaluation\u001b[38;5;241m=\u001b[39meval_config,\n\u001b[1;32m     25\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m---> 26\u001b[0m         project_name\u001b[38;5;241m=\u001b[39m\u001b[43meval_run_name\u001b[49m,\n\u001b[1;32m     27\u001b[0m         project_metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchain\u001b[39m\u001b[38;5;124m\"\u001b[39m: project_name},\n\u001b[1;32m     28\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_run_name' is not defined"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.client import Client\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"baseline\": chain,\n",
    "}\n",
    "\n",
    "run_id = uuid.uuid4().hex[:4]\n",
    "test_runs = {}\n",
    "for project_name, chain in chain_map.items():\n",
    "    test_runs[project_name] = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=lambda: (lambda x: x[\"question\"]) | chain,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=f\"{run_id}-{project_name}\",\n",
    "        project_metadata={\"chain\": project_name},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9429cde-a0f8-4e22-9e3a-5f4171febaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
