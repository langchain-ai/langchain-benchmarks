{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf338c7-557b-420e-ae99-3868f3febfaa",
   "metadata": {},
   "source": [
    "# Semi-structured retrieval\n",
    "\n",
    "We will test retrival of table information from the `Semi-structured Reports`dataset using various methods.\n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da64ebf7-1b59-404f-9a76-b79893f2240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kubernetes 28.1.0 requires urllib3<2.0,>=1.24.2, but you have urllib3 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "types-requests 2.31.0.10 requires urllib3>=2, but you have urllib3 1.26.18 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -U langchain langsmith langchainhub  langchain_benchmarks\n",
    "%pip install --quiet chromadb openai \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684707be-0b26-417f-92d5-5e11fc49a923",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "### Get Public Dataset\n",
    "\n",
    "Get the LangSmith public dataset for semi-structured data, `Semi-structured Reports`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c213899-73dd-4043-b331-c15d818e59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_benchmarks.rag.tasks.semi_structured_reports import get_file_names\n",
    "\n",
    "# Task\n",
    "task = registry[\"Semi-structured Reports\"]\n",
    "\n",
    "# Files used\n",
    "paths = list(get_file_names())\n",
    "files = [str(p) for p in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fc5fc-e8c9-4086-8aa8-37a2eb401cc1",
   "metadata": {},
   "source": [
    "### Base Case\n",
    "\n",
    "Use PDF loader, which is naive to tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23948b39-e22d-4233-bc4c-1173a0bded0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 text elements\n",
      "There are 11 text elements\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def load_and_split(file):\n",
    "    \"\"\"\n",
    "    Load and split PDF files\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFLoader(file)\n",
    "    pdf_pages = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=2000, chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    # Get chunks\n",
    "    docs = text_splitter.split_documents(pdf_pages)\n",
    "    texts = [d.page_content for d in docs]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "baseline_texts = []\n",
    "for fi in files:\n",
    "    baseline_texts.extend(load_and_split(fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcb645-763a-44b6-883e-de55fe8df811",
   "metadata": {},
   "source": [
    "### Unstructured\n",
    "\n",
    "Use tabble-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n",
    "\n",
    "In addition to the below pip packages, you will also need [poppler](https://pdf2image.readthedocs.io/en/latest/installation.html) and [tesseract](https://tesseract-ocr.github.io/tessdoc/Installation.html) in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d2fbe3-c834-4119-9d53-bb3881b3d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/langchain-benchmarks-YXtTxfsL-py3.9/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 tables\n",
      "There are 5 text elements\n",
      "There are 9 tables\n",
      "There are 11 text elements\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(str(element))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "def parse_pdf_unstructured(file):\n",
    "    # Get elements\n",
    "    unstructured_elements = partition_pdf(\n",
    "        filename=file,\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=os.path.dirname(file),\n",
    "    )\n",
    "\n",
    "    # Categorize elements by type\n",
    "    texts, tables = categorize_elements(unstructured_elements)\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "unstructured_texts = []\n",
    "unstructured_tables = []\n",
    "for fi in files:\n",
    "    texts, tables = parse_pdf_unstructured(fi)\n",
    "    unstructured_texts.extend(texts)\n",
    "    unstructured_tables.extend(tables)\n",
    "\n",
    "# Optional: Enforce a specific token size for texts\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(unstructured_texts)\n",
    "unstructured_texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1432f-654d-4b0c-9f92-14c7cada9237",
   "metadata": {},
   "source": [
    "### Docugami\n",
    "\n",
    "Use table-aware splitting following cookbook [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/docugami_xml_kg_rag.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b57330a-0b43-4135-9901-05002ce09af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet -U docugami==0.0.7 dgml-utils==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e09cbb-5758-4ff9-bce5-80730ab2313c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3Q23_Earnings_Press_Release.pdf': '/tmp/tmp9hkzc6m0',\n",
       " 'IF10055.pdf': '/tmp/tmphttq2fts'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docugami import Docugami, upload_to_named_docset, wait_for_dgml\n",
    "\n",
    "# Load\n",
    "DOCSET_NAME = \"Semi-Structured\"\n",
    "\n",
    "dg_client = Docugami()\n",
    "dg_docs = upload_to_named_docset(dg_client, files, DOCSET_NAME)\n",
    "dgml_paths = wait_for_dgml(dg_client, dg_docs)\n",
    "\n",
    "dgml_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799ca7a2-3907-456a-be4c-7de6a43c930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 tables\n",
      "There are 5 text elements\n",
      "There are 9 tables\n",
      "There are 6 text elements\n"
     ]
    }
   ],
   "source": [
    "from dgml_utils.segmentation import get_chunks_str\n",
    "\n",
    "def extract_docugami_file(dgml_path):\n",
    "    with open(dgml_path, \"r\") as file:\n",
    "        contents = file.read().encode(\"utf-8\")\n",
    "\n",
    "        # Chars to OpenAI token math ref:\n",
    "        # https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "        chunks = get_chunks_str(\n",
    "            contents,\n",
    "            min_text_length=1024 * 2,  # 2k chars are ~0.5k tokens\n",
    "            max_text_length=1024 * 8,  # 8k chars are ~2k tokens\n",
    "        )\n",
    "\n",
    "    # Tables\n",
    "    tables = [c.text for c in chunks if \"table\" in c.structure.split()]\n",
    "    print(f\"There are {len(tables)} tables\")\n",
    "\n",
    "    # Text\n",
    "    texts = [c.text for c in chunks if \"table\" not in c.structure.split()]\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts, tables\n",
    "\n",
    "\n",
    "docugami_texts = []\n",
    "docugami_tables = []\n",
    "for fname in files:\n",
    "    # Get xml\n",
    "    dgml_path = dgml_paths[Path(fname).name]\n",
    "    # Extract elelemtns\n",
    "    texts, tables = extract_docugami_file(dgml_path)\n",
    "    docugami_texts.extend(texts)\n",
    "    docugami_tables.extend(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587675d9-c68e-4691-8a6c-796a1c113e6e",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "### Base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80fc789b-1be0-4944-8e8d-a55d2cbe7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore_baseline = Chroma.from_texts(\n",
    "    texts=baseline_texts, collection_name=\"baseline\", embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_baseline = vectorstore_baseline.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab563f-b52e-4bf8-8efb-d16f0fe5179b",
   "metadata": {},
   "source": [
    "### Multi-vector retriever w/ text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1580144d-4931-4163-be64-8612d2e4e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries=None,\n",
    "    texts=None,\n",
    "    table_summaries=None,\n",
    "    tables=None,\n",
    "    image_summaries=None,\n",
    "    images=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10967672-aa36-4405-9d85-701e28d79940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unstructured table extraction + MV Retriever\n",
    "\n",
    "# Get text, table summaries\n",
    "unstructured_text_summaries, unstructured_table_summaries = generate_text_summaries(\n",
    "    unstructured_texts_4k_token, unstructured_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "unstructured_vectorstore = Chroma(\n",
    "    collection_name=\"unstructured\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_unstructured = create_multi_vector_retriever(\n",
    "    unstructured_vectorstore,\n",
    "    unstructured_text_summaries,\n",
    "    unstructured_texts_4k_token,\n",
    "    unstructured_table_summaries,\n",
    "    unstructured_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18c4ce47-f059-40e0-ad2e-530e4f07d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Docugami table extraction + MV Retriever\n",
    "\n",
    "# Get text, table summaries\n",
    "docugami_text_summaries, docugami_table_summaries = generate_text_summaries(\n",
    "    docugami_texts, docugami_tables, summarize_texts=False\n",
    ")\n",
    "\n",
    "# The vectorstore to use to index the summaries\n",
    "docugami_vectorstore = Chroma(\n",
    "    collection_name=\"docugami\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_docugami = create_multi_vector_retriever(\n",
    "    docugami_vectorstore,\n",
    "    docugami_text_summaries,\n",
    "    docugami_texts,\n",
    "    docugami_table_summaries,\n",
    "    docugami_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e1c11-16ad-44e8-9d1b-6e20bf14d524",
   "metadata": {},
   "source": [
    "### Multi-vector retriever w/ images\n",
    "\n",
    "Images extracted from the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb420ba-e2e8-4540-b9f5-57ad605ae5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain.schema.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f47e8-13bc-49b2-8271-681fd2fdc073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp: Store images from papers here\n",
    "base_directory = \"/Users/rlm/Desktop/semi_structured_reports/\"\n",
    "img_base64_list, image_summaries = generate_img_summaries(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6be622-b272-46bd-bd23-590e21c0a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the summaries\n",
    "multi_modal_vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    multi_modal_vectorstore,\n",
    "    image_summaries=image_summaries,\n",
    "    images=img_base64_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94ea8f-eb08-492b-90db-96e27ccd5b61",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcc530b9-c804-4354-967f-4342431eccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Create RAG chains\n",
    "chain_baseline = rag_chain(retriever_baseline)\n",
    "chain_unstructured = rag_chain(retriever_unstructured)\n",
    "chain_docugami = rag_chain(retriever_docugami)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43352e05-ac07-4130-be85-602c1b0f3f40",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6465d2-f770-4ea3-bd9d-7d8419200917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an analyst tasking with providing advice basded on images.\\n\"\n",
    "            \"You may be given a mix of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide analysis related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "chain_multimodal = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b9c4a-ed00-4437-acc6-13f8cb832e81",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c1456-3b26-42d8-bc91-36f52082e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"Analyzing the operating expenses for Q3 2023, which category saw the largest increase when compared to Q3 2022?\"\n",
    "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=6)\n",
    "\n",
    "# Check that we get back relevant images\n",
    "plt_img_base64(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebcbab-3207-4012-82b2-9d71d0f0edb1",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "See guide [here](https://github.com/langchain-ai/langchain-benchmarks/blob/main/docs/source/notebooks/retrieval/semi_structured.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdfdd7bc-a95b-4107-9164-70af9ff8ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Semi-structured Reports already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/530c4d06-5640-4c0f-94fe-0be7b769531f/datasets/98e4bed5-25d9-4599-8e39-1ac7063fc530.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(task.dataset_id, \n",
    "                     dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d729313-5f39-4a4d-87d3-8cd837ad7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'baseline_3b690efd-b54a-44ef-85b1-f0a24db04cf1' at:\n",
      "https://smith.langchain.com/o/530c4d06-5640-4c0f-94fe-0be7b769531f/projects/p/f74bcd0e-4000-431b-9263-a10dc9c6eb67?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-structured Reports at:\n",
      "https://smith.langchain.com/o/530c4d06-5640-4c0f-94fe-0be7b769531f/datasets/98e4bed5-25d9-4599-8e39-1ac7063fc530\n",
      "[------------------------------------------------->] 5/5\n",
      " Eval quantiles:\n",
      "                               0.25       0.5      0.75      mean      mode\n",
      "embedding_cosine_distance  0.013885  0.027287  0.031408  0.026772  0.000006\n",
      "score_string:accuracy      1.000000  1.000000  1.000000  0.940000  1.000000\n",
      "faithfulness               1.000000  1.000000  1.000000  1.000000  1.000000\n",
      "execution_time             7.360070  7.360070  7.360070  7.360070  7.360070\n",
      "View the evaluation results for project 'unstructured_3b690efd-b54a-44ef-85b1-f0a24db04cf1' at:\n",
      "https://smith.langchain.com/o/530c4d06-5640-4c0f-94fe-0be7b769531f/projects/p/a74b6d01-0d14-4f51-85b1-aec3d850bd58?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-structured Reports at:\n",
      "https://smith.langchain.com/o/530c4d06-5640-4c0f-94fe-0be7b769531f/datasets/98e4bed5-25d9-4599-8e39-1ac7063fc530\n",
      "[------------------------------------------------->] 5/5\n",
      " Eval quantiles:\n",
      "                              0.25       0.5      0.75      mean          mode\n",
      "embedding_cosine_distance  0.01455  0.025187  0.027809  0.025195 -2.220446e-16\n",
      "faithfulness               1.00000  1.000000  1.000000  1.000000  1.000000e+00\n",
      "score_string:accuracy      0.70000  1.000000  1.000000  0.880000  1.000000e+00\n",
      "execution_time             5.45546  5.455460  5.455460  5.455460  5.455460e+00\n",
      "View the evaluation results for project 'docugami_3b690efd-b54a-44ef-85b1-f0a24db04cf1' at:\n",
      "https://smith.langchain.com/o/530c4d06-5640-4c0f-94fe-0be7b769531f/projects/p/d9ca4e52-213f-4f8d-a8e9-e7da94006978?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-structured Reports at:\n",
      "https://smith.langchain.com/o/530c4d06-5640-4c0f-94fe-0be7b769531f/datasets/98e4bed5-25d9-4599-8e39-1ac7063fc530\n",
      "[------------------------------------------------->] 5/5\n",
      " Eval quantiles:\n",
      "                               0.25       0.5      0.75      mean      mode\n",
      "embedding_cosine_distance  0.013546  0.025699  0.030151  0.026134  0.000001\n",
      "score_string:accuracy      1.000000  1.000000  1.000000  0.940000  1.000000\n",
      "faithfulness               1.000000  1.000000  1.000000  1.000000  1.000000\n",
      "execution_time             7.801604  7.801604  7.801604  7.801604  7.801604\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config\n",
    "\n",
    "\n",
    "def run_eval(chain, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    test_run = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=lambda: (lambda x: x[\"question\"]) | chain,\n",
    "        evaluation=get_eval_config(),\n",
    "        verbose=True,\n",
    "        project_name = eval_run_name\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"baseline\": chain_baseline,\n",
    "    \"unstructured\": chain_unstructured,\n",
    "    # \"multi_modal\": chain_multimodal,\n",
    "    \"docugami\": chain_docugami,\n",
    "}\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "for project_name, chain in chain_map.items():\n",
    "    run_eval(chain, project_name+\"_\"+run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
