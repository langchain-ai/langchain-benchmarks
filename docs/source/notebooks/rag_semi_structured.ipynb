{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bb467d-861d-4b07-a48d-8e5aa177c969",
   "metadata": {},
   "source": [
    "# Semi-structured RAG\n",
    "\n",
    "Let's evaluate your architecture on a small semi-structured Q&A dataset. This dataset is composed of QA pairs over pdfs that contain tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49db759-7ce6-4ab7-a58f-7fc3a6a7c8ec",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "We will install quite a few prerequisites for this example since we are comparing various techinques and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain_benchmarks\n",
    "# %pip install -U langchain langsmith langchainhub unstructured chromadb openai huggingface pandas langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1631daae-e1ab-4008-84b0-da5937d3b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae13f6-cd40-41e6-bd02-bd683e91cbff",
   "metadata": {},
   "source": [
    "For this code to work, please configure LangSmith environment variables with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b518cf-99fb-44be-8acb-ee0a8ba62272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\n",
    "    \"LANGCHAIN_ENDPOINT\"\n",
    "] = \"http://localhost:1984\"  # \"https://api.smith.langchain.com\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"sk-...\"  # Your API key\n",
    "\n",
    "# Silence warnings from HuggingFace\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a666d-8bf5-4bfd-8b20-8b7defdb8cd5",
   "metadata": {},
   "source": [
    "## Review Q&A Tasks\n",
    "\n",
    "The registry provides configurations to test out common architectures on curated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39159d0-9ea1-414f-a9d8-4a7b22b3d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3644d211-382e-41aa-b282-21b01d28fc35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                            </th><th>Dataset ID                                                               </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Tool Usage - Typewriter (1 func)</td><td>placeholder                                                              </td><td>Environment with a single function that accepts a single letter as input, and &quot;prints&quot; it on a piece of paper.\n",
       "\n",
       "The objective of this task is to evaluate the ability to use the provided tools to repeat a given input string.\n",
       "\n",
       "For example, if the string is &#x27;abc&#x27;, the tools &#x27;a&#x27;, &#x27;b&#x27;, and &#x27;c&#x27; must be invoked in that order.\n",
       "\n",
       "The dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.              </td></tr>\n",
       "<tr><td>Tool Usage - Typewriter         </td><td>placeholder                                                              </td><td>Environment with 26 functions each representing a letter of the alphabet.\n",
       "\n",
       "In this variation of the typewriter task, there are 26 parameterless functions, where each function represents a letter of the alphabet (instead of a single function that takes a letter as an argument).\n",
       "\n",
       "The object is to evaluate the ability of use the functions to repeat the given string.\n",
       "\n",
       "For example, if the string is &#x27;abc&#x27;, the tools &#x27;a&#x27;, &#x27;b&#x27;, and &#x27;c&#x27; must be invoked in that order.\n",
       "\n",
       "The dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.              </td></tr>\n",
       "<tr><td>Tool Usage - Relational Data    </td><td>e95d45da-aaa3-44b3-ba2b-7c15ff6e46f5                                     </td><td>Environment with fake data about users and their locations and favorite foods.\n",
       "\n",
       "The environment provides a set of tools that can be used to query the data.\n",
       "\n",
       "The objective of this task is to evaluate the ability to use the provided tools to answer questions about relational data.\n",
       "\n",
       "The dataset contains 21 examples of varying difficulty. The difficulty is measured by the number of tools that need to be used to answer the question.\n",
       "\n",
       "Each example is composed of a question, a reference answer, and information about the sequence in which tools should be used to answer the question.\n",
       "\n",
       "Success is measured by the ability to answer the question correctly, and efficiently.              </td></tr>\n",
       "<tr><td>Multiverse Math                 </td><td>placeholder                                                              </td><td>An environment that contains a few basic math operations, but with altered results.\n",
       "\n",
       "For example, multiplication of 5*3 will be re-interpreted as 5*3*1.1. The basic operations retain some basic properties, such as commutativity, associativity, and distributivity; however, the results are different than expected.\n",
       "\n",
       "The objective of this task is to evaluate the ability to use the provided tools to solve simple math questions and ignore any innate knowledge about math.              </td></tr>\n",
       "<tr><td>Email Extraction                </td><td>https://smith.langchain.com/public/36bdfe7d-3cd1-4b36-b957-d12d95810a2b/d</td><td>A dataset of 42 real emails deduped from a spam folder, with semantic HTML tags removed, as well as a script for initial extraction and formatting of other emails from an arbitrary .mbox file like the one exported by Gmail.\n",
       "\n",
       "Some additional cleanup of the data was done by hand after the initial pass.\n",
       "\n",
       "See https://github.com/jacoblee93/oss-model-extraction-evals.              </td></tr>\n",
       "<tr><td>LangChain Docs Q&amp;A              </td><td>452ccafc-18e1-4314-885b-edd735f17b9d                                     </td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model&#x27;s response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Earnings        </td><td>c47d9617-ab99-4d6e-a6e6-92b8daf85a7d                                     </td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model&#x27;s response relative to the retrieved documents (if any).              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[ToolUsageTask(name='Tool Usage - Typewriter (1 func)', dataset_id='placeholder', description='Environment with a single function that accepts a single letter as input, and \"prints\" it on a piece of paper.\\n\\nThe objective of this task is to evaluate the ability to use the provided tools to repeat a given input string.\\n\\nFor example, if the string is \\'abc\\', the tools \\'a\\', \\'b\\', and \\'c\\' must be invoked in that order.\\n\\nThe dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.\\n', create_environment=<function get_environment at 0x177a40860>, instructions=\"Repeat the given string by using the provided tools. Do not write anything else or provide any explanations. For example, if the string is 'abc', you must invoke the tools 'a', 'b', and 'c' in that order. Please invoke the function with a single letter at a time.\"), ToolUsageTask(name='Tool Usage - Typewriter', dataset_id='placeholder', description=\"Environment with 26 functions each representing a letter of the alphabet.\\n\\nIn this variation of the typewriter task, there are 26 parameterless functions, where each function represents a letter of the alphabet (instead of a single function that takes a letter as an argument).\\n\\nThe object is to evaluate the ability of use the functions to repeat the given string.\\n\\nFor example, if the string is 'abc', the tools 'a', 'b', and 'c' must be invoked in that order.\\n\\nThe dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.\\n\", create_environment=<function get_environment at 0x177a40ae0>, instructions=\"Repeat the given string by using the provided tools. Do not write anything else or provide any explanations. For example, if the string is 'abc', you must invoke the tools 'a', 'b', and 'c' in that order. Please invoke the functions without any arguments.\"), ToolUsageTask(name='Tool Usage - Relational Data', dataset_id='e95d45da-aaa3-44b3-ba2b-7c15ff6e46f5', description='Environment with fake data about users and their locations and favorite foods.\\n\\nThe environment provides a set of tools that can be used to query the data.\\n\\nThe objective of this task is to evaluate the ability to use the provided tools to answer questions about relational data.\\n\\nThe dataset contains 21 examples of varying difficulty. The difficulty is measured by the number of tools that need to be used to answer the question.\\n\\nEach example is composed of a question, a reference answer, and information about the sequence in which tools should be used to answer the question.\\n\\nSuccess is measured by the ability to answer the question correctly, and efficiently.\\n', create_environment=<function get_environment at 0x177a402c0>, instructions=\"Please answer the user's question by using the tools provided. Do not guess the answer. Keep in mind that entities like users,foods and locations have both a name and an ID, which are not the same.\"), ToolUsageTask(name='Multiverse Math', dataset_id='placeholder', description='An environment that contains a few basic math operations, but with altered results.\\n\\nFor example, multiplication of 5*3 will be re-interpreted as 5*3*1.1. The basic operations retain some basic properties, such as commutativity, associativity, and distributivity; however, the results are different than expected.\\n\\nThe objective of this task is to evaluate the ability to use the provided tools to solve simple math questions and ignore any innate knowledge about math.\\n', create_environment=<function get_environment at 0x1779ffd80>, instructions='You are requested to solve math questions in an alternate mathematical universe. The rules of association, commutativity, and distributivity still apply, but the operations have been altered to yield different results than expected. Solve the given math questions using the provided tools. Do not guess the answer.'), ExtractionTask(name='Email Extraction', dataset_id='https://smith.langchain.com/public/36bdfe7d-3cd1-4b36-b957-d12d95810a2b/d', description='A dataset of 42 real emails deduped from a spam folder, with semantic HTML tags removed, as well as a script for initial extraction and formatting of other emails from an arbitrary .mbox file like the one exported by Gmail.\\n\\nSome additional cleanup of the data was done by hand after the initial pass.\\n\\nSee https://github.com/jacoblee93/oss-model-extraction-evals.\\n    ', model=<class 'langchain_benchmarks.extraction.email_task.Email'>), RetrievalTask(name='LangChain Docs Q&A', dataset_id='452ccafc-18e1-4314-885b-edd735f17b9d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x1779ff060>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x1779ff100>, 'hyde': <function _chroma_hyde_retriever_factory at 0x1779ff1a0>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x10fc30cc0>}, get_docs=functools.partial(<function load_docs_from_parquet at 0x1779fefc0>, '/Users/wfh/code/lc/langchain-benchmarks/langchain_benchmarks/rag/tasks/langchain_docs/indexing/db_docs/docs.parquet')), RetrievalTask(name='Semi-structured Earnings', dataset_id='c47d9617-ab99-4d6e-a6e6-92b8daf85a7d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x1779ff880>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x1779ff920>, 'hyde': <function _chroma_hyde_retriever_factory at 0x1779ff9c0>}, architecture_factories={}, get_docs=<function load_docs at 0x1779ff7e0>)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671282f8-c455-4390-b018-e53bbd833093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>Semi-structured Earnings            </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                       </td></tr>\n",
       "<tr><td>Dataset ID            </td><td>c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</td></tr>\n",
       "<tr><td>Description           </td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model&#x27;s response relative to the retrieved documents (if any).                                     </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>basic, parent-doc, hyde             </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>                                    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='Semi-structured Earnings', dataset_id='c47d9617-ab99-4d6e-a6e6-92b8daf85a7d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x1779ff880>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x1779ff920>, 'hyde': <function _chroma_hyde_retriever_factory at 0x1779ff9c0>}, architecture_factories={}, get_docs=<function load_docs at 0x1779ff7e0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = registry[\"Semi-structured Earnings\"]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70369f67-deb4-467a-801a-6d38c3d0460d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Semi-structured Earnings already exists. Skipping.\n",
      "You can access the dataset at http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/df5a7c83-35d1-4cad-973b-dde6d70e1735.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(task.dataset_id, dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4fafb2-63d0-40b4-b803-0095c5b22ca6",
   "metadata": {},
   "source": [
    "### Now, index the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ad4b23-fbd4-4ebc-b5a5-d3d05efd0b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e63911aa0e4ad3ba25409bd0822fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-base\")\n",
    "docs = list(task.get_docs())\n",
    "retriever_factory = task.retriever_factories[\"basic\"]\n",
    "# Indexes the documents with the specified embeddings\n",
    "retriever = retriever_factory(embeddings, docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efac89-12f9-47e3-b60f-65d9279ebc1e",
   "metadata": {},
   "source": [
    "### Time to evaluate\n",
    "\n",
    "We will compose our retriever with a simple Llama based LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d1bc360-d822-43a8-b6b7-ff66dc27caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer based solely on the retrieved documents below:\\n\\n<Documents>\\n{docs}</Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"{Question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatAnthropic(model=\"claude-2\")\n",
    "\n",
    "\n",
    "def create_chain(retriever):\n",
    "    return (\n",
    "        RunnableAssign({\"docs\": (lambda x: next(iter(x.values()))) | retriever})\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "935cacd9-e841-4c76-ac16-f3f0cf18df62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-earnest-stretch-74' at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/projects/p/2f2c0482-06b2-4219-98d7-3ce64ac621e9?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-structured Earnings at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/df5a7c83-35d1-4cad-973b-dde6d70e1735\n",
      "[------------------------------------------------->] 5/5\n",
      " Eval quantiles:\n",
      "                                          inputs.Question  \\\n",
      "count                                                   5   \n",
      "unique                                                  5   \n",
      "top     Analyzing the operating expenses for Q3 2023, ...   \n",
      "freq                                                    1   \n",
      "mean                                                  NaN   \n",
      "std                                                   NaN   \n",
      "min                                                   NaN   \n",
      "25%                                                   NaN   \n",
      "50%                                                   NaN   \n",
      "75%                                                   NaN   \n",
      "max                                                   NaN   \n",
      "\n",
      "       feedback.faithfulness  feedback.embedding_cosine_distance  \\\n",
      "count                      0                            5.000000   \n",
      "unique                     0                                 NaN   \n",
      "top                      NaN                                 NaN   \n",
      "freq                     NaN                                 NaN   \n",
      "mean                     NaN                            0.066968   \n",
      "std                      NaN                            0.022299   \n",
      "min                      NaN                            0.040185   \n",
      "25%                      NaN                            0.050600   \n",
      "50%                      NaN                            0.073105   \n",
      "75%                      NaN                            0.073621   \n",
      "max                      NaN                            0.097329   \n",
      "\n",
      "        feedback.score_string:accuracy error  execution_time  \n",
      "count                          5.00000     0        5.000000  \n",
      "unique                             NaN     0             NaN  \n",
      "top                                NaN   NaN             NaN  \n",
      "freq                               NaN   NaN             NaN  \n",
      "mean                           0.64000   NaN        6.988965  \n",
      "std                            0.49295   NaN        3.075064  \n",
      "min                            0.10000   NaN        4.254811  \n",
      "25%                            0.10000   NaN        5.081688  \n",
      "50%                            1.00000   NaN        5.296321  \n",
      "75%                            1.00000   NaN        8.780231  \n",
      "max                            1.00000   NaN       11.531772  \n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain_benchmarks.rag import get_eval_config\n",
    "from langsmith.client import Client\n",
    "\n",
    "client = Client()\n",
    "RAG_EVALUATION = get_eval_config()\n",
    "chain = create_chain(retriever)\n",
    "test_run = client.run_on_dataset(\n",
    "    dataset_name=task.name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b54ef6c-0194-410a-aae9-f30c2097548a",
   "metadata": {},
   "source": [
    "## Example processing the docs\n",
    "\n",
    "RAG apps are as good as the information they are able to retrieve. Let's try indexing the tables' summaries to\n",
    "improve the likelihood that they are retrieved whenever a user asks a relevant question.\n",
    "\n",
    "We will use unstructured's `partition_pdf` functionality and generate summaries using an LLM.\n",
    "\n",
    "You can define your own indexing pipeline to see how it impacts the downstream performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3378eddb-0a8d-4179-8e9c-54343469eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are summarizing semi-structured tables or text in a pdf.\\n\\n```document\\n{doc}\\n```\",\n",
    "        ),\n",
    "        (\"user\", \"Write a concise summary.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "\n",
    "def create_doc(x) -> Document:\n",
    "    return Document(\n",
    "        page_content=x[\"output\"],\n",
    "        metadata=x[\"doc\"].metadata,\n",
    "    )\n",
    "\n",
    "\n",
    "summarize_chain = (\n",
    "    {\"doc\": lambda x: x}\n",
    "    | RunnableAssign({\"prompt\": prompt})\n",
    "    | {\n",
    "        \"output\": itemgetter(\"prompt\") | model | StrOutputParser(),\n",
    "        \"doc\": itemgetter(\"doc\"),\n",
    "    }\n",
    "    | create_doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a2f070-3b5a-4de0-b3da-ddfb6e6f8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarize_chain.batch(\n",
    "    [doc for doc in docs if doc.metadata[\"element_type\"] == \"table\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc0bf8-fa50-4be3-8d23-04f6129548e0",
   "metadata": {},
   "source": [
    "Index the documents and create the retriever. We will re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35a1ccf6-2c2f-46f2-838e-5a5bf89515f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6993b803411485fb8460c8924dd8fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Indexes the documents with the specified embeddings\n",
    "retriever_with_summaries = retriever_factory(\n",
    "    embeddings,\n",
    "    docs=docs + summaries,\n",
    "    # Specify a unique transformation name to avoid local cache collisions with other indices.\n",
    "    transformation_name=\"table-summaries\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821e4b0-8e67-418a-840c-470fcde42df0",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "We'll evaluate the new chain on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f08c4c-a738-4449-9190-5a4f0b65b99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-terrific-smile-42' at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/projects/p/ec94b1ee-c37f-42b9-a141-4a5997dd7efb?eval=true\n",
      "\n",
      "View all tests for Dataset Semi-structured Earnings at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/df5a7c83-35d1-4cad-973b-dde6d70e1735\n",
      "[------------------------------------------------->] 5/5"
     ]
    }
   ],
   "source": [
    "chain_2 = create_chain(retriever_with_summaries)\n",
    "\n",
    "test_run_with_summaries = client.run_on_dataset(\n",
    "    dataset_name=task.name,\n",
    "    llm_or_chain_factory=chain_2,\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a96dbc-9a38-4d65-87bd-1e027d2215dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
