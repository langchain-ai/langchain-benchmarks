{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba9f105-c48f-4d8c-8253-355ef13156b0",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "Let's benchmark against all tool usage tasks. \n",
    "\n",
    "Expand the models list to benchmark with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7483b-d08f-49fa-83da-619863171e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks import (\n",
    "    __version__,\n",
    "    clone_public_dataset,\n",
    "    model_registry,\n",
    "    registry,\n",
    ")\n",
    "from langchain_benchmarks.rate_limiting import RateLimiter\n",
    "from langchain_benchmarks.tool_usage.agents import (\n",
    "    AnthropicToolUserFactory,\n",
    "    CustomAgentFactory,\n",
    "    OpenAIAgentFactory,\n",
    "    OpenAIAssistantFactory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbe23b-a3b1-4607-929d-ea6e88b7085e",
   "metadata": {},
   "source": [
    "Prior to starting the tests, you may want to verify\n",
    "that the task that you're working with and the models are propelry defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2bb0c-c741-4fb4-96bc-54b3ee88bf5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = registry[\"Multiverse Math\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbcaa9-349c-4223-89be-4abff9cf76ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_factory = OpenAIAgentFactory(\n",
    "    task, model=\"mistral-7b-instruct-v0.1\"\n",
    ")  # Follows OpenAI function format\n",
    "# agent_factory = OpenAIAgentFactory(task, model='gpt-3.5-turbo-1106')\n",
    "agent_factory().invoke({\"question\": \"(2 + 5) and then to the power of 0.5\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165f3a1-4e70-4caa-b082-78d4e0c56410",
   "metadata": {},
   "source": [
    "Let's make an experiment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d7695-416c-4faf-8c33-c40e5f136672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_uuid = \"woof\"  # Or generate ranom using uuid.uuid4().hex[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bde1e0-f9f7-47f1-816a-d01a28577544",
   "metadata": {},
   "source": [
    "Define the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d390b6-9ade-424c-aabb-d450f52ed121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tests = [\n",
    "    # 2-tuple of (architecture, model name)\n",
    "    (\"anthropic_tool_user\", \"claude-2.1\"),\n",
    "    (\"openai_functions\", \"mistral-7b-instruct-v0.1\"),\n",
    "    (\"openai_functions\", \"gpt-3.5-turbo-1106\"),\n",
    "    (\"openai_functions\", \"gpt-3.5-turbo-0613\"),\n",
    "    (\"openai_functions\", \"gpt-4-1106-preview\"),\n",
    "    (\"openai_functions\", \"gpt-4-0613\"),\n",
    "    (\"openai_assistant\", \"gpt-4-1106-preview\"),\n",
    "    (\"openai_assistant\", \"gpt-3.5-turbo-1106\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b7c24-8b4d-4bd7-8b00-365fbe61897f",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1894b-6232-421c-a243-567617cba083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbc3ef-7a3f-430f-8b79-45af5861b3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client()  # Launch langsmith client for cloning datasets\n",
    "today = datetime.date.today().isoformat()\n",
    "rate_limiter = RateLimiter(requests_per_second=2)\n",
    "\n",
    "for task in registry:\n",
    "    if task.type != \"ToolUsageTask\":\n",
    "        continue\n",
    "\n",
    "    dataset_name = task.name\n",
    "    clone_public_dataset(task.dataset_id, dataset_name=dataset_name)\n",
    "\n",
    "    for arch, model in tests:\n",
    "        print()\n",
    "        print(f\"Benchmarking {task.name} with model: {model} and arch: {arch}\")\n",
    "        eval_config = task.get_eval_config()\n",
    "\n",
    "        if arch == \"openai_functions\":\n",
    "            agent_factory = OpenAIAgentFactory(\n",
    "                task, model=model, rate_limiter=rate_limiter\n",
    "            )\n",
    "        elif arch == \"custom_agent\":\n",
    "            agent_factory = CustomAgentFactory(\n",
    "                task, model=model, rate_limiter=rate_limiter\n",
    "            )\n",
    "        elif arch == \"anthropic_tool_user\":\n",
    "            agent_factory = AnthropicToolUserFactory(task)\n",
    "        elif arch == \"openai_assistant\":\n",
    "            agent_factory = OpenAIAssistantFactory(task, model=model)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        client.run_on_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            llm_or_chain_factory=agent_factory,\n",
    "            evaluation=eval_config,\n",
    "            verbose=False,\n",
    "            project_name=f\"{model}-{task.name}-{today}-{experiment_uuid}\",\n",
    "            tags=[model],\n",
    "            concurrency_level=5,\n",
    "            project_metadata={\n",
    "                \"model\": model,\n",
    "                \"id\": experiment_uuid,\n",
    "                \"task\": task.name,\n",
    "                \"date\": today,\n",
    "                \"langchain_benchmarks_version\": __version__,\n",
    "                \"arch\": arch,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7999f-e8ab-45a6-88a9-0ae76f3d24cf",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818572a-a5fb-4153-bbe0-6f9e90813a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langsmith.client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7890951-ffde-4706-95e5-ae3e9bf0e8a6",
   "metadata": {},
   "source": [
    "Let's fetch all the data that has the same experiment ID and place it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44822aa4-8c4e-46be-8126-b79a9acdf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ids = [\"3f3e\"]\n",
    "\n",
    "\n",
    "def _endswith(s, suffixes):\n",
    "    return any(s.endswith(suffix) for suffix in suffixes)\n",
    "\n",
    "\n",
    "client = Client()\n",
    "projects = [\n",
    "    project\n",
    "    for project in client.list_projects()\n",
    "    if _endswith(project.name, experiment_ids)\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for project in projects:\n",
    "    # Temporary way to get tag information\n",
    "    project_info = client.read_project(project_id=project.id)\n",
    "    try:\n",
    "        test_results = client.get_test_results(project_name=project.name)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    for k, v in project_info.extra[\"metadata\"].items():\n",
    "        test_results[k] = v\n",
    "\n",
    "    dfs.append(test_results)\n",
    "\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065b7a0-d514-49f7-9d79-67181c41f56d",
   "metadata": {},
   "source": [
    "Compute a standardized \"correct\" column. It uses \"Correct Final State\" for tool usage tasks, and \"correctness (which is based on output) for the other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0466a-25f4-44d7-bd2a-20da51461994",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "\n",
    "for r in df.to_dict(orient=\"records\"):\n",
    "    if \"Typewriter\" in r[\"task\"]:\n",
    "        correct.append(r[\"feedback.Correct Final State\"])\n",
    "    else:\n",
    "        correct.append(r[\"feedback.correctness\"])\n",
    "\n",
    "df[\"correct\"] = correct\n",
    "df[\"correct\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b8ae9-c84b-4ebc-88ab-fa0ac5e28a57",
   "metadata": {},
   "source": [
    "Compute some statistics. We're using estimating standard error of the mean assuming a bernoulli process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d080c-d3ac-43c3-a527-9961913db2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = df.groupby([\"model\", \"task\"])[\"correct\"].sum().to_frame(\"num_correct\")\n",
    "total = df.groupby([\"task\", \"model\"]).size().to_frame(\"total\")\n",
    "stats_df = total.join(num_correct)\n",
    "stats_df[\"% correct\"] = stats_df[\"num_correct\"] / stats_df[\"total\"]\n",
    "stats_df[\"error\"] = np.sqrt(\n",
    "    stats_df[\"% correct\"] * (1 - stats_df[\"% correct\"]) / stats_df[\"total\"]\n",
    ")\n",
    "\n",
    "# stats_df\n",
    "\n",
    "models = [\n",
    "    \"llama-v2-70b-chat-fw\",\n",
    "    \"mixtral-8x7b-instruct-fw\",\n",
    "    \"claude-2\",\n",
    "    \"claude-2.1\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo-1106\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4-1106-preview\",\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"Tool Usage - Typewriter (1 tool)\",\n",
    "    \"Tool Usage - Typewriter (26 tools)\",\n",
    "    \"Multiverse Math\",\n",
    "    \"Tool Usage - Relational Data\",\n",
    "]\n",
    "\n",
    "stats_df = stats_df.reset_index()\n",
    "stats_df = stats_df[stats_df[\"model\"].isin(models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f79af-128c-4e2e-8c1e-807e397b9791",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df66a1-960c-40a3-abc8-58b503fceda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(tasks))  # the label locations\n",
    "width = 0.08  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout=\"constrained\", figsize=(16, 4))\n",
    "colormap = plt.get_cmap(\"Set2\").colors\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    try:\n",
    "        results = stats_df.set_index(\"model\").loc[model]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    color = colormap[idx]\n",
    "\n",
    "    results = results.set_index(\"task\").loc[tasks]\n",
    "    measurement = results[\"% correct\"]\n",
    "\n",
    "    values = [round(m, 2) for m in measurement]\n",
    "\n",
    "    offset = width * multiplier * 1.4\n",
    "    rects = ax.bar(\n",
    "        x + offset, values, width, label=model, yerr=results[\"error\"], color=color\n",
    "    )\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel(\"% Questions Answered Correctly\")\n",
    "ax.set_title(\"Tool Usage Performance\")\n",
    "ax.set_xticks(x + width + 0.3, tasks)\n",
    "ax.legend(\n",
    "    loc=\"center left\", ncols=1, bbox_to_anchor=(1.0, 0.5), frameon=False, title=\"Model\"\n",
    ")\n",
    "ax.set_ylim(0, 1.10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
