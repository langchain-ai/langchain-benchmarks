{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aae613b-6adb-4e6f-bae7-4974358e07aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark All Tasks\n",
    "\n",
    "Let's benchmark against all tool usage tasks. \n",
    "\n",
    "Expand the `test` list to benchmark with different models and agent architectures.\n",
    "\n",
    "Note that this requires `langsmith>=0.0.72` to run the viz parts at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525d100-b612-4118-af91-6bdc4aa3fb38",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "\n",
    "### Credentials\n",
    "\n",
    "First, let's set up the models to be tested and the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "387c494b-ad7e-452e-8d11-0d5d28db855c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# This is just the default list below\n",
    "required_env_vars = [\n",
    "    \"LANGCHAIN_API_KEY\",\n",
    "    \"ANTHROPIC_API_KEY\",\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"MISTRAL_API_KEY\",\n",
    "]\n",
    "for var in required_env_vars:\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass(f\"Provide the required {var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e54ab-ebbe-4b9a-a596-facae66e1ced",
   "metadata": {},
   "source": [
    "### Instantiate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a4e40a-5850-4a0b-b9af-36e9c8b55e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_benchmarks.tool_usage.agents import StandardAgentFactory\n",
    "\n",
    "tests = [\n",
    "    (\n",
    "        \"gemini-1.0-pro-002\",\n",
    "        ChatVertexAI(model_name=\"gemini-1.0-pro-002\", temperature=0),\n",
    "    ),\n",
    "    (\n",
    "        \"gemini-1.5-pro-preview-0409\",\n",
    "        ChatVertexAI(model_name=\"gemini-1.5-pro-preview-0409\", temperature=0),\n",
    "    ),\n",
    "    (\n",
    "        \"open-mixtral-8x22b-2404\",\n",
    "        ChatMistralAI(model=\"open-mixtral-8x22b-2404\", temperature=0),\n",
    "    ),\n",
    "    (\"mistral-large-2402\", ChatMistralAI(model=\"mistral-large-2402\", temperature=0)),\n",
    "    (\n",
    "        \"claude-3-opus-20240229\",\n",
    "        ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0),\n",
    "    ),\n",
    "    (\n",
    "        \"claude-3-haiku-20240307\",\n",
    "        ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0),\n",
    "    ),\n",
    "    (\n",
    "        \"claude-3-sonnet-20240229\",\n",
    "        ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0),\n",
    "    ),\n",
    "    (\"gpt-3.5-turbo-0125\", ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)),\n",
    "    (\n",
    "        \"gpt-4-turbo-2024-04-09\",\n",
    "        ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\", temperature=0),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308c18a-209c-44f8-b762-7a07851101f2",
   "metadata": {},
   "source": [
    "### Set up the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e152e2e-1fb1-4918-9a53-0744c0ef0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks import (\n",
    "    __version__,\n",
    "    clone_public_dataset,\n",
    "    model_registry,\n",
    "    registry,\n",
    ")\n",
    "from langchain_benchmarks.rate_limiting import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e4664d-00a1-473b-ae83-f2435962971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompts for the agents\n",
    "# Using two prompts because some chat models do not support SystemMessage.\n",
    "without_system_message_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{instructions}\\n{question}\",\n",
    "        ),  # Populated from task.instructions automatically\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),  # Workspace for the agent\n",
    "    ]\n",
    ")\n",
    "\n",
    "with_system_message_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"{instructions}\"),\n",
    "        (\"human\", \"{question}\"),  # Populated from task.instructions automatically\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),  # Workspace for the agent\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165f3a1-4e70-4caa-b082-78d4e0c56410",
   "metadata": {},
   "source": [
    "Generate an experiment id.\n",
    "\n",
    "We can tag our runs with this experiment ID and pull data from LangSmith using this experiment ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066d7695-416c-4faf-8c33-c40e5f136672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_uuid = \"sky25\"  # Or generate ranom using uuid.uuid4().hex[:4]\n",
    "# experiment_uuid = uuid.uuid4().hex[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125aad7-cac7-4ec7-9c18-98defe9d2236",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4c45e-88a6-4c96-ba5d-cfaf03905789",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()  # Launch langsmith client for cloning datasets\n",
    "today = datetime.date.today().isoformat()\n",
    "\n",
    "\n",
    "for task in registry.tasks:\n",
    "    if task.type != \"ToolUsageTask\":\n",
    "        continue\n",
    "\n",
    "    # This is a small test dataset that can be used to verify\n",
    "    # that everything is set up correctly prior to running over\n",
    "    # all results. We may remove it in the future.\n",
    "    if task.name == \"Multiverse Math (Tiny)\":\n",
    "        continue\n",
    "\n",
    "    dataset_name = task.name + f\" ({today})\"\n",
    "    clone_public_dataset(task.dataset_id, dataset_name=dataset_name)\n",
    "\n",
    "    for model_name, model in tests:\n",
    "        if model_name.startswith(\"gemini\"):\n",
    "            # google models don't use system prompt\n",
    "            prompt = without_system_message_prompt\n",
    "            rate_limiter = RateLimiter(requests_per_second=0.1)\n",
    "        else:\n",
    "            prompt = with_system_message_prompt\n",
    "            rate_limiter = RateLimiter(requests_per_second=1)\n",
    "        print()\n",
    "        print(f\"Benchmarking {task.name} with model: {model_name}\")\n",
    "        eval_config = task.get_eval_config()\n",
    "\n",
    "        agent_factory = StandardAgentFactory(\n",
    "            task, model, prompt, rate_limiter=rate_limiter\n",
    "        )\n",
    "\n",
    "        client.run_on_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            llm_or_chain_factory=agent_factory,\n",
    "            evaluation=eval_config,\n",
    "            verbose=False,\n",
    "            project_name=f\"{model_name}-{task.name}-{today}-{experiment_uuid}\",\n",
    "            concurrency_level=5,\n",
    "            project_metadata={\n",
    "                \"model\": model_name,\n",
    "                \"id\": experiment_uuid,\n",
    "                \"task\": task.name,\n",
    "                \"date\": today,\n",
    "                \"langchain_benchmarks_version\": __version__,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7999f-e8ab-45a6-88a9-0ae76f3d24cf",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "\n",
    "Note that if the queue is under significant load, you may want to wait before running the following to ensure all runs are in the DB and all stats are correctly computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b92f0-7d64-4731-b294-05948d4db562",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7818572a-a5fb-4153-bbe0-6f9e90813a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langsmith.client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7890951-ffde-4706-95e5-ae3e9bf0e8a6",
   "metadata": {},
   "source": [
    "Let's fetch all the data that has the same experiment ID and place it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44822aa4-8c4e-46be-8126-b79a9acdf8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_ids = [\"sky25\"]\n",
    "dataset_names = [\n",
    "    \"Tool Usage - Typewriter (1 tool)\",\n",
    "    \"Tool Usage - Typewriter (26 tools)\",\n",
    "    \"Tool Usage - Relational Data\",\n",
    "    \"Multiverse Math\",\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "projects = []\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_name_ = dataset_name + f\" ({today})\"\n",
    "    for project in client.list_projects(reference_dataset_name=dataset_name_):\n",
    "        if (\n",
    "            project.metadata.get(\"id\") in experiment_ids\n",
    "            and project.end_time is not None\n",
    "        ):\n",
    "            projects.append(project)\n",
    "\n",
    "dfs = []\n",
    "keys = set()\n",
    "for project in projects:\n",
    "    # Temporary way to get tag information\n",
    "    try:\n",
    "        test_results = client.get_test_results(project_name=project.name)\n",
    "    except Exception as e:\n",
    "        print(e, project.run_count)\n",
    "        continue\n",
    "\n",
    "    for k, v in project.metadata.items():\n",
    "        test_results[k] = v\n",
    "    keys.update(test_results.columns)\n",
    "    dfs.append(test_results)\n",
    "for df in dfs:\n",
    "    missing = list(keys - set(df.columns))\n",
    "    for key in missing:\n",
    "        df[key] = None\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065b7a0-d514-49f7-9d79-67181c41f56d",
   "metadata": {},
   "source": [
    "Compute a standardized \"correct\" column. It uses \"Correct Final State\" for tool usage tasks, and \"correctness (which is based on output) for the other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3c0466a-25f4-44d7-bd2a-20da51461994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = []\n",
    "\n",
    "for r in df.to_dict(orient=\"records\"):\n",
    "    if \"Typewriter\" in r[\"task\"]:\n",
    "        correct.append(r[\"feedback.correct final state\"])\n",
    "    else:\n",
    "        correct.append(r[\"feedback.correctness\"])\n",
    "\n",
    "df[\"correct\"] = correct\n",
    "df[\"correct\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b8ae9-c84b-4ebc-88ab-fa0ac5e28a57",
   "metadata": {},
   "source": [
    "Compute some statistics. We're using estimating standard error of the mean assuming a bernoulli process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c59d080c-d3ac-43c3-a527-9961913db2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_correct = df.groupby([\"model\", \"task\"])[\"correct\"].sum().to_frame(\"num_correct\")\n",
    "total = df.groupby([\"task\", \"model\"]).size().to_frame(\"total\")\n",
    "stats_df = total.join(num_correct)\n",
    "stats_df[\"% correct\"] = stats_df[\"num_correct\"] / stats_df[\"total\"]\n",
    "stats_df[\"error\"] = np.sqrt(\n",
    "    stats_df[\"% correct\"] * (1 - stats_df[\"% correct\"]) / stats_df[\"total\"]\n",
    ")\n",
    "\n",
    "tasks = [\n",
    "    \"Tool Usage - Typewriter (1 tool)\",\n",
    "    \"Tool Usage - Typewriter (26 tools)\",\n",
    "    \"Multiverse Math\",\n",
    "    \"Tool Usage - Relational Data\",\n",
    "]\n",
    "\n",
    "stats_df = stats_df.reset_index()\n",
    "models = stats_df[\"model\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdbd6005-906a-42fd-af05-b4f27e2c3c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['claude-3-haiku-20240307', 'claude-3-opus-20240229',\n",
       "       'claude-3-sonnet-20240229', 'gemini-1.0-pro-002',\n",
       "       'gemini-1.5-pro-preview-0409', 'gpt-3.5-turbo-0125',\n",
       "       'gpt-4-turbo-2024-04-09', 'mistral-large-2402',\n",
       "       'open-mixtral-8x22b-2404'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f79af-128c-4e2e-8c1e-807e397b9791",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df66a1-960c-40a3-abc8-58b503fceda5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "x = np.arange(len(tasks))  # the label locations\n",
    "width = 0.06  # the width of the bars\n",
    "multiplier = 1.1\n",
    "\n",
    "fig, ax = plt.subplots(layout=\"constrained\", figsize=(20, 4))\n",
    "colormap = plt.get_cmap(\"Set3\").colors\n",
    "idx = 0\n",
    "for model in models:\n",
    "    try:\n",
    "        results = stats_df.set_index(\"model\").loc[model]\n",
    "    except:\n",
    "        continue\n",
    "    if len(results) == 0:\n",
    "        continue\n",
    "    color = colormap[idx]\n",
    "    idx += 1\n",
    "\n",
    "    results = results.set_index(\"task\").loc[tasks]\n",
    "    measurement = results[\"% correct\"]\n",
    "\n",
    "    values = [round(m, 2) for m in measurement]\n",
    "\n",
    "    offset = width * multiplier * 1.4\n",
    "    rects = ax.bar(\n",
    "        x + offset,\n",
    "        values,\n",
    "        width,\n",
    "        label=f\"{model}\",\n",
    "        yerr=results[\"error\"],\n",
    "        color=color,\n",
    "    )\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel(\"% Questions Answered Correctly\")\n",
    "ax.set_title(\"Tool Usage Performance\")\n",
    "ax.set_xticks(x + width + 0.3, tasks)\n",
    "ax.legend(\n",
    "    loc=\"center left\", ncols=1, bbox_to_anchor=(1.0, 0.5), frameon=False, title=\"Model\"\n",
    ")\n",
    "ax.set_ylim(0, 1.10)\n",
    "plt.savefig(\"overall_perf.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
