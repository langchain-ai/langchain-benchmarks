{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bb467d-861d-4b07-a48d-8e5aa177c969",
   "metadata": {},
   "source": [
    "# Relational Data \n",
    "\n",
    "In this task, an agent is given access to a set of tools that can be used to make queries across 3 relational tables.\n",
    "\n",
    "The tables contain information about users, locations and foods. The agent must answer questions about the data using the provided tools.\n",
    "\n",
    "The underlying data looks like this (showing first 2 records)\n",
    "\n",
    "User data:\n",
    "\n",
    "|  id  |   name   |       email       | location | favorite_color | favorite_foods |\n",
    "| ---- | -------- | ----------------- | -------- | --------------- | --------------- |\n",
    "|  1   |  Alice   | alice@gmail.com  |    1     |      red        |   [1, 2, 3]    |\n",
    "|  21  |   Bob    | bob@hotmail.com  |    2     |     orange      |   [4, 5, 6]    |\n",
    "\n",
    "\n",
    "Location Data:\n",
    "\n",
    "|  id  |    city      |     current_time      |            current_weather               |\n",
    "| ---- | ------------ | --------------------- | --------------------------------------- |\n",
    "|  1   |  New York    | 2023-11-14 10:30 AM  | Partly Cloudy, Temperature: 68°F       |\n",
    "|  2   | Los Angeles  | 2023-11-14 7:45 AM   | Sunny, Temperature: 75°F               |\n",
    "\n",
    "\n",
    "\n",
    "Food data:\n",
    "\n",
    "|  id  |    name    | calories | allergic_ingredients     |\n",
    "| ---- | ---------- | -------- | ------------------------ |\n",
    "|  1   |   Pizza    |   285    |   [\"Gluten\", \"Dairy\"]    |\n",
    "|  2   | Chocolate  |    50    |   [\"Milk\", \"Soy\"]        |\n",
    "\n",
    "\n",
    "The tools allow to look up information based on ids (e.g., `get_user_email` takes a user id and returns the email),\n",
    "and to search (e.g., `find_foods_by_name` takes a food name and returns a list of results).\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39159d0-9ea1-414f-a9d8-4a7b22b3d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03488ab1-31ed-41c2-8da2-46b02599b181",
   "metadata": {},
   "source": [
    "For this code to work, please configure LangSmith environment variables with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f22779-a948-4833-8e8c-ace9ef17f56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = registry[\"Tool Usage - Relational Data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33a639-3caf-4314-8ea7-1c7c8b1d114d",
   "metadata": {},
   "source": [
    "Clone the dataset associaetd with this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70369f67-deb4-467a-801a-6d38c3d0460d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Tool Usage - Relational Data already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(task.dataset_id, dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bdafa-bdab-4194-90c9-46416d14b2f9",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "Let's check the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b6b0fd-639d-43a7-a730-9acdc5b2f102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='get_user_name', description=\"get_user_name(user_id: int) -> str - Get the name of the user with the given user ID.\\n\\n        Args:\\n            user_id: The user's ID.\\n\\n        Returns:\\n            The user's name.\", args_schema=<class 'pydantic.v1.main.get_user_nameSchemaSchema'>, handle_tool_error=True, func=<function get_available_functions.<locals>.get_user_name at 0x7fbb0e864f40>),\n",
       " StructuredTool(name='list_user_ids', description='list_user_ids() -> List[str] - List all the user IDs.', args_schema=<class 'pydantic.v1.main.list_user_idsSchemaSchema'>, handle_tool_error=True, func=<function get_available_functions.<locals>.list_user_ids at 0x7fbb0e864fe0>),\n",
       " StructuredTool(name='find_users_by_name', description='find_users_by_name(name: str) -> List[langchain_benchmarks.tool_usage.tasks.relational_data.SearchHit] - Find users with the given name.\\n\\n        Args:\\n            name: The name to search for.\\n\\n        Returns:\\n            The list of matching users.', args_schema=<class 'pydantic.v1.main.find_users_by_nameSchemaSchema'>, handle_tool_error=True, func=<function get_available_functions.<locals>.find_users_by_name at 0x7fbb0e865080>),\n",
       " StructuredTool(name='find_locations_by_name', description='find_locations_by_name(city: str) -> List[langchain_benchmarks.tool_usage.tasks.relational_data.SearchHit] - Find locations with the given city name.', args_schema=<class 'pydantic.v1.main.find_locations_by_nameSchemaSchema'>, handle_tool_error=True, func=<function get_available_functions.<locals>.find_locations_by_name at 0x7fbb0e865120>),\n",
       " StructuredTool(name='find_foods_by_name', description='find_foods_by_name(food: str) -> List[langchain_benchmarks.tool_usage.tasks.relational_data.SearchHit] - Find foods with the given name.', args_schema=<class 'pydantic.v1.main.find_foods_by_nameSchemaSchema'>, handle_tool_error=True, func=<function get_available_functions.<locals>.find_foods_by_name at 0x7fbb0e8651c0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = task.create_environment()\n",
    "env.tools[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1c1242-449c-4536-863d-b62bf6d2dff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.tools[0].invoke({\"user_id\": 21})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854e139b-a120-4012-bdf4-6394e0b1c42d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 2, 'city': 'Los Angeles'},\n",
       " {'id': 1, 'city': 'New York'},\n",
       " {'id': 3, 'city': 'Chicago'},\n",
       " {'id': 4, 'city': 'Houston'},\n",
       " {'id': 5, 'city': 'Miami'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.tools[3].invoke({\"city\": \"LA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462f7b8-fd42-4613-ab5f-5f3cbbc37d28",
   "metadata": {},
   "source": [
    "## Agent Factory\n",
    "\n",
    "For evaluation, we need an agent factory that will create a new instance of an agent executor for every evaluation run.\n",
    "\n",
    "The `AgentExecutor` should accept `question` as an input and include the fields `output`, `intermediate_steps` and potentially `state` in its response -- for this we\n",
    "will wrap the agent executor in an adapter (`apply_agent_executor_adapter`) that will help match the expected schema.\n",
    "\n",
    "Please reference the LangChain documentation to see how to [use and implement agents](https://python.langchain.com/docs/modules/agents/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2d80d2-4ddf-4b80-b6c5-331133a85314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, Tool, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_benchmarks.schema import ExtractionTask\n",
    "from langchain_benchmarks.tool_usage.agents import apply_agent_executor_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c0e4a1-f56e-4117-8804-4161c642b068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgentFactory:\n",
    "    def __init__(self, task: ExtractionTask, model: str) -> None:\n",
    "        self.task = task\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self):\n",
    "        # This factory creates a new environment for every agent run.\n",
    "        # The reason is that the environment may be associated with an environment state (e.g., typewriter)\n",
    "        # which is changed by the actions of the agent.\n",
    "        # At the end of the run, the environment state will be read.\n",
    "        env = task.create_environment()  # Create a new environment for every agent run!\n",
    "        tools = env.tools\n",
    "        llm = ChatOpenAI(temperature=0, model=self.model)\n",
    "        agent_executor = initialize_agent(\n",
    "            tools,\n",
    "            llm,\n",
    "            agent=AgentType.OPENAI_FUNCTIONS,\n",
    "            return_intermediate_steps=True,\n",
    "            handle_parsing_errors=True,\n",
    "        )\n",
    "        # Apply the adapters so that inputs and outputs match dataset schema\n",
    "        # state_reader automatically adds the state of the environment at the end of the run.\n",
    "        return apply_agent_executor_adapter(agent_executor, state_reader=env.read_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae8c6be-899c-44a6-a89b-0fc04c2cb05c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = [\"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-0613\", \"gpt-4-32k-0613\"]\n",
    "agent_factory = AgentFactory(task, models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a64f76-65ae-4367-b43f-f2be3431e7af",
   "metadata": {},
   "source": [
    "Let's test that our agent works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127a8aa5-839c-469c-a870-7b498f37c187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import globals\n",
    "\n",
    "globals.set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e4896fa-3633-44a1-857f-80a263cf2e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `find_locations_by_name` with `{'city': 'Los Angeles'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'id': 2, 'city': 'Los Angeles'}, {'id': 4, 'city': 'Houston'}, {'id': 1, 'city': 'New York'}, {'id': 3, 'city': 'Chicago'}, {'id': 5, 'city': 'Miami'}]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_weather_at_location` with `{'location_id': 2}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSunny, Temperature: 75°F\u001b[0m\u001b[32;1m\u001b[1;3mThe weather in Los Angeles is sunny with a temperature of 75°F.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the weather in LA?',\n",
       " 'output': 'The weather in Los Angeles is sunny with a temperature of 75°F.',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='find_locations_by_name', tool_input={'city': 'Los Angeles'}, log=\"\\nInvoking: `find_locations_by_name` with `{'city': 'Los Angeles'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"city\":\"Los Angeles\"}', 'name': 'find_locations_by_name'}})]),\n",
       "   [{'id': 2, 'city': 'Los Angeles'},\n",
       "    {'id': 4, 'city': 'Houston'},\n",
       "    {'id': 1, 'city': 'New York'},\n",
       "    {'id': 3, 'city': 'Chicago'},\n",
       "    {'id': 5, 'city': 'Miami'}]),\n",
       "  (AgentActionMessageLog(tool='get_weather_at_location', tool_input={'location_id': 2}, log=\"\\nInvoking: `get_weather_at_location` with `{'location_id': 2}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location_id\":2}', 'name': 'get_weather_at_location'}})]),\n",
       "   'Sunny, Temperature: 75°F')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = agent_factory()\n",
    "agent.invoke({\"question\": \"whats the weather in LA?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43edee23-109d-4f75-be68-d2b4b3240c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "globals.set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821e4b0-8e67-418a-840c-470fcde42df0",
   "metadata": {},
   "source": [
    "## Eval\n",
    "\n",
    "Let's evaluate an agent now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e02fb65-eecf-43b8-bf76-1e86ca535da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View the evaluation results for project 'tool-usage-relational-data-gpt-3.5-turbo-1106-8258' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/8aae8e36-720a-42c8-8540-5d5475e7181e?eval=true\n",
      "\n",
      "View all tests for Dataset Tool Usage - Relational Data at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826\n",
      "[------------------------------------------------->] 21/21\n",
      "View the evaluation results for project 'tool-usage-relational-data-gpt-3.5-turbo-0613-8258' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/d8773df1-b054-41e4-a947-7b256ca8738b?eval=true\n",
      "\n",
      "View all tests for Dataset Tool Usage - Relational Data at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826\n",
      "[------------------------------------------------->] 21/21\n",
      "View the evaluation results for project 'tool-usage-relational-data-gpt-4-0613-8258' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/090fecae-923f-4281-93f7-2c5253a2a2a4?eval=true\n",
      "\n",
      "View all tests for Dataset Tool Usage - Relational Data at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826\n",
      "[------------------------------------------------->] 21/21"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.tool_usage import get_eval_config\n",
    "\n",
    "experiment_uuid = uuid.uuid4().hex[:4]\n",
    "\n",
    "client = Client()\n",
    "\n",
    "models = [\"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "\n",
    "for model in models:\n",
    "    print()\n",
    "    eval_config = get_eval_config()\n",
    "    agent_factory = AgentFactory(task, model=model)\n",
    "    test_run = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=agent_factory,\n",
    "        evaluation=eval_config,\n",
    "        verbose=False,\n",
    "        project_name=f\"tool-usage-relational-data-{model}-{experiment_uuid}\",\n",
    "        tags=[model],\n",
    "        project_metadata={\n",
    "            \"model\": model,\n",
    "            \"arch\": \"openai-functions-agent\",\n",
    "            \"id\": experiment_uuid,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b039225-01cf-481a-87a6-4e880e9b1dcd",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "\n",
    "Here, we'll take a look at the underlying results a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cd3bb99-9078-43d0-8b22-6e4d6917929c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith.client import Client\n",
    "\n",
    "client = Client()\n",
    "projects = list(client.list_projects(reference_dataset_name=\"Multiverse Math\"))\n",
    "\n",
    "dfs = []\n",
    "for project in projects:\n",
    "    first_root_run = next(\n",
    "        client.list_runs(project_name=project.name, execution_order=1)\n",
    "    )\n",
    "    # Temporary way to get tag information\n",
    "    tags = first_root_run.tags\n",
    "    test_results = client.get_test_results(project_name=project.name)\n",
    "    test_results[\"model\"] = tags[0]\n",
    "    dfs.append(test_results)\n",
    "\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca74c84c-8d48-4cc1-bb37-65ba95e082b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.Intermediate steps correctness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>feedback.# steps / # expected steps</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0613</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8.308014</td>\n",
       "      <td>1.03333</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-1106</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>54.992810</td>\n",
       "      <td>0.93332</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8.554704</td>\n",
       "      <td>0.79999</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feedback.correctness  \\\n",
       "model                                      \n",
       "gpt-3.5-turbo-0613                   0.3   \n",
       "gpt-3.5-turbo-1106                   0.1   \n",
       "gpt-4-0613                           0.0   \n",
       "\n",
       "                    feedback.Intermediate steps correctness  execution_time  \\\n",
       "model                                                                         \n",
       "gpt-3.5-turbo-0613                                      0.8        8.308014   \n",
       "gpt-3.5-turbo-1106                                      0.7       54.992810   \n",
       "gpt-4-0613                                              0.6        8.554704   \n",
       "\n",
       "                    feedback.# steps / # expected steps   n  \n",
       "model                                                        \n",
       "gpt-3.5-turbo-0613                              1.03333  10  \n",
       "gpt-3.5-turbo-1106                              0.93332  10  \n",
       "gpt-4-0613                                      0.79999  10  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df = df.groupby(\"model\").size().to_frame(\"n\")\n",
    "df.groupby(\"model\")[\n",
    "    [\n",
    "        \"feedback.correctness\",\n",
    "        \"feedback.Intermediate steps correctness\",\n",
    "        \"execution_time\",\n",
    "        \"feedback.# steps / # expected steps\",\n",
    "    ]\n",
    "].mean().join(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe9b20c4-9da0-47a2-95a3-b5660a54855a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith.client import Client\n",
    "\n",
    "client = Client()\n",
    "projects = list(\n",
    "    client.list_projects(reference_dataset_name=\"Tool Usage - Relational Data\")\n",
    ")\n",
    "\n",
    "dfs = []\n",
    "for project in projects:\n",
    "    first_root_run = next(\n",
    "        client.list_runs(project_name=project.name, execution_order=1)\n",
    "    )\n",
    "    # Temporary way to get tag information\n",
    "    tags = first_root_run.tags\n",
    "    test_results = client.get_test_results(project_name=project.name)\n",
    "    test_results[\"model\"] = tags[0]\n",
    "    dfs.append(test_results)\n",
    "\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d366a-8754-417a-a654-956528f134e2",
   "metadata": {},
   "source": [
    "In terms of function usage, gpt-4 uses more calls than is strictly necessary (`feedback.# steps / # expected steps` is > 1). However, it's doing a pretty good job.\n",
    "\n",
    "The gpt-3.5 models do not use tools enough (`feedback.# steps / # expected steps` is < 1) and as a result do a worse job at the task.\n",
    "\n",
    "Note: The intermediate step correctness happens to have the same average for the 3 models -- this is just a coincidence you can confirm by inspecting underlying results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "066551f2-eb30-4bc1-94fd-0ca0085103ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.Intermediate steps correctness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>feedback.# steps / # expected steps</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0613</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>4.829506</td>\n",
       "      <td>0.825390</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-1106</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>5.464218</td>\n",
       "      <td>0.965871</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>8.544358</td>\n",
       "      <td>1.037300</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feedback.correctness  \\\n",
       "model                                      \n",
       "gpt-3.5-turbo-0613              0.714286   \n",
       "gpt-3.5-turbo-1106              0.857143   \n",
       "gpt-4-0613                      0.952381   \n",
       "\n",
       "                    feedback.Intermediate steps correctness  execution_time  \\\n",
       "model                                                                         \n",
       "gpt-3.5-turbo-0613                                 0.714286        4.829506   \n",
       "gpt-3.5-turbo-1106                                 0.714286        5.464218   \n",
       "gpt-4-0613                                         0.714286        8.544358   \n",
       "\n",
       "                    feedback.# steps / # expected steps   n  \n",
       "model                                                        \n",
       "gpt-3.5-turbo-0613                             0.825390  21  \n",
       "gpt-3.5-turbo-1106                             0.965871  21  \n",
       "gpt-4-0613                                     1.037300  21  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df = df.groupby(\"model\").size().to_frame(\"n\")\n",
    "df.groupby(\"model\")[\n",
    "    [\n",
    "        \"feedback.correctness\",\n",
    "        \"feedback.Intermediate steps correctness\",\n",
    "        \"execution_time\",\n",
    "        \"feedback.# steps / # expected steps\",\n",
    "    ]\n",
    "].mean().join(count_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
