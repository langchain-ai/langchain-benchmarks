{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9df2ed-3496-45c6-8b1b-e12776a02a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Tool Usage tasks are designed to evaluate how well an agent can use tools to accomplish an objective.\n",
    "\n",
    "Each task defines an environment in which the agent operates. The environment consists of a set of tools and a way to read the state of the environment (more on that below).\n",
    "\n",
    "The tasks allow you to stress test the agent in different ways:\n",
    "\n",
    "* Can the agent use a single tool effectively?\n",
    "* Can the agent use more than 10 tools effectively?\n",
    "* Can the agent correctly incorporate information returned by the tool (and ignore internal knowledge)?\n",
    "\n",
    "To help in this evaluation, each task is associated with a LangSmith dataset that includes input/output examples of varying difficulties.\n",
    "\n",
    "## Schema\n",
    "\n",
    "To make it possible to evaluate different agent implementations, we're using a standardized schema, we'll illustrate it with the following example taken from tool usage.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Each task corresponds to a LangSmith dataset with the following schema:\n",
    "\n",
    "Inputs:\n",
    "\n",
    "|     name    |     type    |     meaning            |\n",
    "| ----------- | ----------- | -----------------------|\n",
    "| question    | str         | the user question      |\n",
    "\n",
    "\n",
    "Outputs:\n",
    "\n",
    "|     name      |     type        |     meaning                                            |\n",
    "| ------------- | --------------- | ------------------------------------------------------|\n",
    "| reference     | str             | the expected answer                                   |\n",
    "| expected_steps| List[str]       | the list of tools that should be invoked              |\n",
    "| order_matters | bool            | whether the tools should be invoked in the specific order |\n",
    "| state         | Optional[Any]   | the state of the system after the agent has taken its actions |\n",
    "\n",
    "\n",
    "\n",
    "Here's an [example](https://smith.langchain.com/public/1d89f4b3-5f73-48cf-a127-2fdeb22f6d84/d/e82a0faf-00b9-40a5-a0e3-9723d923e58e/e) contains the following keys/values:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input\": {\"question\": \"weather in LA right now?\"},\n",
    "  \"output\": {\n",
    "      \"reference\": \"Sunny, Temperature: 75Â°F\",\n",
    "      \"order_matters\": true,\n",
    "      \"expected_steps\": [\n",
    "        \"find_locations_by_name\",\n",
    "        \"get_current_weather_for_location\"\n",
    "      ],\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Agent\n",
    "\n",
    "To work with the evaluators provided by LangChain Benchmarks (of course you're free to write your own evaluators!).\n",
    "\n",
    "An agent must accept `question` as an input and return:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"output\": \"It's super sunny. Like 75F\", // the output from the agent\n",
    "    \"intermediate_steps\": [... \"find_locations_by_name\" ...], // list of the intermediate steps taken by the agent (see format in LangChain)\n",
    "    \"state\": .., // Can be anything, this is the state fo the environment after the agent has taken all of its actions (optional key)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478781d-80ad-44ab-a0f8-1fc3d8c0f14d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tasks\n",
    "\n",
    "You can check an up-to-date list of tool usage tasks in the registry:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9b82fc-b689-4a25-b718-99ecc2fc6867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                              </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Tool Usage - Typewriter (1 tool)  </td><td>ToolUsageTask</td><td><a href=\"https://smith.langchain.com/public/59577193-8938-4ccf-92a7-e8a96bcf4f86/d\" target=\"_blank\" rel=\"noopener\">59577193-8938-4ccf-92a7-e8a96bcf4f86</a></td><td>Environment with a single tool that accepts a single letter as input, and prints it on a piece of virtual paper.\n",
       "\n",
       "The objective of this task is to evaluate the ability of the model to use the provided tools to repeat a given input string.\n",
       "\n",
       "For example, if the string is 'abc', the tools 'a', 'b', and 'c' must be invoked in that order.\n",
       "\n",
       "The dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.              </td></tr>\n",
       "<tr><td>Tool Usage - Typewriter (26 tools)</td><td>ToolUsageTask</td><td><a href=\"https://smith.langchain.com/public/128af05e-aa00-4e3b-a958-d166dd450581/d\" target=\"_blank\" rel=\"noopener\">128af05e-aa00-4e3b-a958-d166dd450581</a></td><td>Environment with 26 tools each tool represents a letter of the alphabet.\n",
       "\n",
       "The objective of this task is to evaluate the model's ability the use tools\n",
       "for a simple repetition task.\n",
       "\n",
       "For example, if the string is 'abc', the tools 'a', 'b', and 'c' must be invoked in that order.\n",
       "\n",
       "The dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.\n",
       "\n",
       "This is a variation of the typer writer task, where 26 parameterless tools are\n",
       "given instead of a single tool that takes a letter as an argument.              </td></tr>\n",
       "<tr><td>Tool Usage - Relational Data      </td><td>ToolUsageTask</td><td><a href=\"https://smith.langchain.com/public/1d89f4b3-5f73-48cf-a127-2fdeb22f6d84/d\" target=\"_blank\" rel=\"noopener\">1d89f4b3-5f73-48cf-a127-2fdeb22f6d84</a></td><td>Environment with fake data about users and their locations and favorite foods.\n",
       "\n",
       "The environment provides a set of tools that can be used to query the data.\n",
       "\n",
       "The objective of this task is to evaluate the ability to use the provided tools to answer questions about relational data.\n",
       "\n",
       "The dataset contains 21 examples of varying difficulty. The difficulty is measured by the number of tools that need to be used to answer the question.\n",
       "\n",
       "Each example is composed of a question, a reference answer, and information about the sequence in which tools should be used to answer the question.\n",
       "\n",
       "Success is measured by the ability to answer the question correctly, and efficiently.              </td></tr>\n",
       "<tr><td>Multiverse Math                   </td><td>ToolUsageTask</td><td><a href=\"https://smith.langchain.com/public/594f9f60-30a0-49bf-b075-f44beabf546a/d\" target=\"_blank\" rel=\"noopener\">594f9f60-30a0-49bf-b075-f44beabf546a</a></td><td>An environment that contains a few basic math operations, but with altered results.\n",
       "\n",
       "For example, multiplication of 5*3 will be re-interpreted as 5*3*1.1. The basic operations retain some basic properties, such as commutativity, associativity, and distributivity; however, the results are different than expected.\n",
       "\n",
       "The objective of this task is to evaluate the ability to use the provided tools to solve simple math questions and ignore any innate knowledge about math.              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[ToolUsageTask(name='Tool Usage - Typewriter (1 tool)', dataset_id='https://smith.langchain.com/public/59577193-8938-4ccf-92a7-e8a96bcf4f86/d', description=\"Environment with a single tool that accepts a single letter as input, and prints it on a piece of virtual paper.\\n\\nThe objective of this task is to evaluate the ability of the model to use the provided tools to repeat a given input string.\\n\\nFor example, if the string is 'abc', the tools 'a', 'b', and 'c' must be invoked in that order.\\n\\nThe dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.\\n\", create_environment=<function get_environment at 0x7f490480e520>, instructions=\"Repeat the given string using the provided tools. Do not write anything else or provide any explanations. For example, if the string is 'abc', you must print the letters 'a', 'b', and 'c' one at a time and in that order. \", eval_params={'output_evaluation': 'none'}), ToolUsageTask(name='Tool Usage - Typewriter (26 tools)', dataset_id='https://smith.langchain.com/public/128af05e-aa00-4e3b-a958-d166dd450581/d', description=\"Environment with 26 tools each tool represents a letter of the alphabet.\\n\\nThe objective of this task is to evaluate the model's ability the use tools\\nfor a simple repetition task.\\n\\nFor example, if the string is 'abc', the tools 'a', 'b', and 'c' must be invoked in that order.\\n\\nThe dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.\\n\\nThis is a variation of the typer writer task, where 26 parameterless tools are\\ngiven instead of a single tool that takes a letter as an argument.\\n\", create_environment=<function get_environment at 0x7f490480ea20>, instructions=\"Repeat the given string by using the provided tools. Do not write anything else or provide any explanations. For example, if the string is 'abc', you must invoke the tools 'a', 'b', and 'c' in that order. Please invoke the functions without any arguments.\", eval_params={'output_evaluation': 'none'}), ToolUsageTask(name='Tool Usage - Relational Data', dataset_id='https://smith.langchain.com/public/1d89f4b3-5f73-48cf-a127-2fdeb22f6d84/d', description='Environment with fake data about users and their locations and favorite foods.\\n\\nThe environment provides a set of tools that can be used to query the data.\\n\\nThe objective of this task is to evaluate the ability to use the provided tools to answer questions about relational data.\\n\\nThe dataset contains 21 examples of varying difficulty. The difficulty is measured by the number of tools that need to be used to answer the question.\\n\\nEach example is composed of a question, a reference answer, and information about the sequence in which tools should be used to answer the question.\\n\\nSuccess is measured by the ability to answer the question correctly, and efficiently.\\n', create_environment=<function get_environment at 0x7f490480e020>, instructions=\"Please answer the user's question by using the tools provided. Do not guess the answer. Keep in mind that entities like users,foods and locations have both a name and an ID, which are not the same.\", eval_params={}), ToolUsageTask(name='Multiverse Math', dataset_id='https://smith.langchain.com/public/594f9f60-30a0-49bf-b075-f44beabf546a/d', description='An environment that contains a few basic math operations, but with altered results.\\n\\nFor example, multiplication of 5*3 will be re-interpreted as 5*3*1.1. The basic operations retain some basic properties, such as commutativity, associativity, and distributivity; however, the results are different than expected.\\n\\nThe objective of this task is to evaluate the ability to use the provided tools to solve simple math questions and ignore any innate knowledge about math.\\n', create_environment=<function get_environment at 0x7f490480da80>, instructions='You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Only include the numeric response without any clarifications.', eval_params={'output_evaluation': 'qa_math'})])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_benchmarks import registry\n",
    "\n",
    "registry.filter(Type=\"ToolUsageTask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f54cdd3-67f6-43ba-a929-1a6ed1b01296",
   "metadata": {},
   "source": [
    "Let's understand what a tool usage task is in a bit more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7543739b-d212-4249-9b4a-fc406a58c9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name       </td><td>Tool Usage - Typewriter (26 tools)                                                                                                                         </td></tr>\n",
       "<tr><td>Type       </td><td>ToolUsageTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID </td><td><a href=\"https://smith.langchain.com/public/128af05e-aa00-4e3b-a958-d166dd450581/d\" target=\"_blank\" rel=\"noopener\">128af05e-aa00-4e3b-a958-d166dd450581</a></td></tr>\n",
       "<tr><td>Description</td><td>Environment with 26 tools each tool represents a letter of the alphabet.\n",
       "\n",
       "The objective of this task is to evaluate the model's ability the use tools\n",
       "for a simple repetition task.\n",
       "\n",
       "For example, if the string is 'abc', the tools 'a', 'b', and 'c' must be invoked in that order.\n",
       "\n",
       "The dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.\n",
       "\n",
       "This is a variation of the typer writer task, where 26 parameterless tools are\n",
       "given instead of a single tool that takes a letter as an argument.                                                                                                                                                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "ToolUsageTask(name='Tool Usage - Typewriter (26 tools)', dataset_id='https://smith.langchain.com/public/128af05e-aa00-4e3b-a958-d166dd450581/d', description=\"Environment with 26 tools each tool represents a letter of the alphabet.\\n\\nThe objective of this task is to evaluate the model's ability the use tools\\nfor a simple repetition task.\\n\\nFor example, if the string is 'abc', the tools 'a', 'b', and 'c' must be invoked in that order.\\n\\nThe dataset includes examples of varying difficulty. The difficulty is measured by the length of the string.\\n\\nThis is a variation of the typer writer task, where 26 parameterless tools are\\ngiven instead of a single tool that takes a letter as an argument.\\n\", create_environment=<function get_environment at 0x7f490480ea20>, instructions=\"Repeat the given string by using the provided tools. Do not write anything else or provide any explanations. For example, if the string is 'abc', you must invoke the tools 'a', 'b', and 'c' in that order. Please invoke the functions without any arguments.\", eval_params={'output_evaluation': 'none'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = registry[\"Tool Usage - Typewriter (26 tools)\"]\n",
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d6c77-8ed0-41d0-a7f5-303356cca4af",
   "metadata": {},
   "source": [
    "Tool usage tasks are associated with an environment\n",
    "\n",
    "---------\n",
    "```python\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ToolUsageEnvironment:\n",
    "    \"\"\"An instance of an environment for tool usage.\"\"\"\n",
    "\n",
    "    tools: List[BaseTool]\n",
    "    \"\"\"The tools that can be used in the environment.\"\"\"\n",
    "\n",
    "    read_state: Optional[Callable[[], Any]] = None\n",
    "    \"\"\"A function that returns the current state of the environment.\"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b99c2-82b1-4eb5-87cb-5891d0b16a4b",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "Here, we'll dig into the typewriter task a bit to explain what the environment state represents.\n",
    "\n",
    "The typewrite task has 26 tools each of which prints a letter on a piece of virtual paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f201dbbe-7d92-4bc7-b4b5-ea8901dd2970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='a', description='a() -> str - Run to Type the letter \"a\".', args_schema=<class 'pydantic.v1.main.aSchemaSchema'>, func=<function _create_typing_func.<locals>.func at 0x7f490480fe20>),\n",
       " StructuredTool(name='b', description='b() -> str - Run to Type the letter \"b\".', args_schema=<class 'pydantic.v1.main.bSchemaSchema'>, func=<function _create_typing_func.<locals>.func at 0x7f490480fec0>),\n",
       " StructuredTool(name='c', description='c() -> str - Run to Type the letter \"c\".', args_schema=<class 'pydantic.v1.main.cSchemaSchema'>, func=<function _create_typing_func.<locals>.func at 0x7f490480ff60>),\n",
       " StructuredTool(name='d', description='d() -> str - Run to Type the letter \"d\".', args_schema=<class 'pydantic.v1.main.dSchemaSchema'>, func=<function _create_typing_func.<locals>.func at 0x7f4904844040>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = task.create_environment()\n",
    "env.tools[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07957ee-ae52-47d4-a4ff-aa99d4d9bdaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.tools[0].invoke({})  # Invoke a()\n",
    "env.tools[0].invoke({})  # invoke a()\n",
    "env.tools[2].invoke({})  # invoke c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fbb9b6-00f6-4445-b480-00eed6b5b3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aac'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.read_state()  # Shows the content of the virtual paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39b9b3-d4da-49bc-b3db-8a4165b1db55",
   "metadata": {},
   "source": [
    "## Agent Factory\n",
    "\n",
    "For evaluation, we need an agent factory that will create a new instance of an agent executor for every evaluation run.\n",
    "\n",
    "The `AgentExecutor` should accept `question` as an input and include the fields `output`, `intermediate_steps` and potentially `state` in its response -- for this we\n",
    "will wrap the agent executor in an adapter (`apply_agent_executor_adapter`) that will help match the expected schema.\n",
    "\n",
    "Please reference the LangChain documentation to see how to [use and implement agents](https://python.langchain.com/docs/modules/agents/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca8ad69-9956-451c-b639-ea30c77d982f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_benchmarks.schema import ExtractionTask\n",
    "from langchain_benchmarks.tool_usage.agents import apply_agent_executor_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44839ebe-48ea-4d5b-87b4-2ad72acacb71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AgentFactory:\n",
    "    def __init__(self, task: ExtractionTask, model: str) -> None:\n",
    "        self.task = task\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self):\n",
    "        # This factory creates a new environment for every agent run.\n",
    "        # The reason is that the environment may be associated with an environment state (e.g., typewriter)\n",
    "        # which is changed by the actions of the agent.\n",
    "        # At the end of the run, the environment state will be read.\n",
    "        env = task.create_environment()  # Create a new environment for every agent run!\n",
    "        tools = env.tools\n",
    "        llm = ChatOpenAI(temperature=0, model=self.model)\n",
    "        agent_executor = initialize_agent(\n",
    "            tools,\n",
    "            llm,\n",
    "            agent=AgentType.OPENAI_FUNCTIONS,\n",
    "            return_intermediate_steps=True,\n",
    "            handle_parsing_errors=True,\n",
    "        )\n",
    "        # Apply the adapters so that inputs and outputs match dataset schema\n",
    "        # state_reader automatically adds the state of the environment at the end of the run.\n",
    "        return apply_agent_executor_adapter(agent_executor, state_reader=env.read_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755f7920-831b-4595-8c6d-cca22c935198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import globals\n",
    "\n",
    "globals.set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b18952b-43b8-4f30-a0d9-e7763eb05b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_factory = AgentFactory(task, model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a81e5-b3d6-42e5-895d-0c4dc8413738",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's check that the agent works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2804eae-5b0b-4a38-9dff-363a4fe8f324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = agent_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0bb2bf-5f53-4f59-a73f-2144fe850d50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `a` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mOK\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `b` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mOK\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `c` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mOK\u001b[0m\u001b[32;1m\u001b[1;3mYou've successfully typed \"abc\"! Is there anything else you'd like to do?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'abc',\n",
       " 'output': 'You\\'ve successfully typed \"abc\"! Is there anything else you\\'d like to do?',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='a', tool_input={}, log='\\nInvoking: `a` with `{}`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{}', 'name': 'a'}})]),\n",
       "   'OK'),\n",
       "  (AgentActionMessageLog(tool='b', tool_input={}, log='\\nInvoking: `b` with `{}`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{}', 'name': 'b'}})]),\n",
       "   'OK'),\n",
       "  (AgentActionMessageLog(tool='c', tool_input={}, log='\\nInvoking: `c` with `{}`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{}', 'name': 'c'}})]),\n",
       "   'OK')],\n",
       " 'state': 'abc'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"question\": \"abc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bce984-7c9c-4f6e-a51b-01c3e2b6e00a",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "How does one evaluate an agent? Given a particular task and input, an agent uses tools to produce an output AND/OR change the state of the environment.\n",
    "\n",
    "To evaluate an agent, we can check the following:\n",
    "\n",
    "1. Did the agent use the expected tools?\n",
    "2. Did the agent use the tools in the most effective way; e.g., was the order of tool invocation correct?\n",
    "3. Did the environment end up in the correct final state after the agent used the tools? (e.g., does my calendar contain all the scheduled meetings?)\n",
    "4. Did the agent output match the expected reference output?\n",
    "\n",
    "Each task is associated with a standard evaluator that does evaluation that's appropriate for the task; for example,\n",
    "\n",
    "1. Use an LLM to grade Compare output to reference using an LLM that grades the response.\n",
    "2. Compare equality of expected_steps to the list of tools in intermediate_steps -- simple list equality\n",
    "3. Compare the state of the environment against expected state (if present in the dataset and in the agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e5817-3b9d-4a1e-8ee8-692d39aa68ca",
   "metadata": {},
   "source": [
    "This evaluator will be used below when we benchmark on all tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88bd6e1-f77e-4668-a143-096929e897ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunEvalConfig(evaluators=[], custom_evaluators=[<langchain_benchmarks.tool_usage.evaluators.AgentTrajectoryEvaluator object at 0x7f49003b9990>], reference_key=None, prediction_key=None, input_key=None, eval_llm=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_config = task.get_eval_config()\n",
    "eval_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c7f91-9bb3-44b5-802d-f9f444ddeff9",
   "metadata": {},
   "source": [
    "Set up code to run against all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60466447-eb37-4204-a497-fe47e8d8dd70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks import (\n",
    "    __version__,\n",
    "    clone_public_dataset,\n",
    "    model_registry,\n",
    "    registry,\n",
    ")\n",
    "from langchain_benchmarks.rate_limiting import RateLimiter\n",
    "from langchain_benchmarks.tool_usage.agents import (\n",
    "    AnthropicToolUserFactory,\n",
    "    CustomAgentFactory,\n",
    "    OpenAIAgentFactory,\n",
    "    OpenAIAssistantFactory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c448d139-9923-4cf6-af49-cbf3dff46bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "experiment_uuid = uuid.uuid4().hex[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "200df769-4dd9-453b-8500-219c1d5305f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tests = [\n",
    "    # 2-tuple of (architecture, model name)\n",
    "    (\"openai_functions\", \"gpt-3.5-turbo-1106\"), # Requires OpenAI Creds\n",
    "    (\"openai_functions\", \"gpt-3.5-turbo-0613\"),\n",
    "    (\"openai_functions\", \"gpt-4-1106-preview\"),\n",
    "    (\"openai_functions\", \"gpt-4-0613\"),\n",
    "    (\"openai_functions\", \"mistral-7b-instruct-v0.1\"), # Requires AnyScale creds\n",
    "    (\"anthropic_tool_user\", \"claude-2.1\"), # Requires Anthropic Creds and Setting up Anthropics Tool Usage package.\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf7355-7db9-4adc-bc1e-f04c3d0ec57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client()  # Launch langsmith client for cloning datasets\n",
    "today = datetime.date.today().isoformat()\n",
    "rate_limiter = RateLimiter(requests_per_second=2)\n",
    "\n",
    "for task in registry:\n",
    "    if task.type != \"ToolUsageTask\":\n",
    "        continue\n",
    "\n",
    "    dataset_name = task.name\n",
    "    clone_public_dataset(task.dataset_id, dataset_name=dataset_name)\n",
    "\n",
    "    for arch, model in tests:\n",
    "        print()\n",
    "        print(f\"Benchmarking {task.name} with model: {model} and arch: {arch}\")\n",
    "        eval_config = task.get_eval_config()\n",
    "\n",
    "        if arch == \"openai_functions\":\n",
    "            agent_factory = OpenAIAgentFactory(\n",
    "                task, model=model, rate_limiter=rate_limiter\n",
    "            )\n",
    "        elif arch == \"custom_agent\":\n",
    "            agent_factory = CustomAgentFactory(\n",
    "                task, model=model, rate_limiter=rate_limiter\n",
    "            )\n",
    "        elif arch == \"anthropic_tool_user\":\n",
    "            agent_factory = AnthropicToolUserFactory(task)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        client.run_on_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            llm_or_chain_factory=agent_factory,\n",
    "            evaluation=eval_config,\n",
    "            verbose=False,\n",
    "            project_name=f\"{model}-{task.name}-{today}-{experiment_uuid}\",\n",
    "            tags=[model],\n",
    "            concurrency_level=5,\n",
    "            project_metadata={\n",
    "                \"model\": model,\n",
    "                \"id\": experiment_uuid,\n",
    "                \"task\": task.name,\n",
    "                \"date\": today,\n",
    "                \"langchain_benchmarks_version\": __version__,\n",
    "                \"arch\": arch,\n",
    "            },\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
