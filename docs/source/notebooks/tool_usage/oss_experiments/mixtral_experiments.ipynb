{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bb467d-861d-4b07-a48d-8e5aa177c969",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating OSS Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b729e-b851-4ab8-a3a9-be34b329b985",
   "metadata": {
    "tags": []
   },
   "source": [
    "For this code to work, please configure LangSmith environment variables with your credentials.\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls_..\"  # Your LangSmith API key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "666a8246-b1a9-47ce-b159-d950692fc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "keys = [\"LANGCHAIN_API_KEY\", \"FIREWORKS_API_KEY\"]\n",
    "for key in keys:\n",
    "    if not os.environ.get(key):\n",
    "        os.environ[key] = getpass(f\"Set {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d65770-6a4f-4029-beba-5fa9aeb18809",
   "metadata": {},
   "source": [
    "## Agent Factory\n",
    "\n",
    "For evaluation, we need an agent factory that will create a new instance of an agent executor for every evaluation run.\n",
    "\n",
    "We'll use an custom AgentFactory provided with LangChain Benchmarks -- look at the `intro` section to see how to define your own.\n",
    "\n",
    "We will use the Fireworks API for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a35cbf20-7632-4116-9c6c-cee6e4a98068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "from langchain.agents import AgentExecutor, AgentType, Tool, initialize_agent\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.structured_chat.output_parser import (\n",
    "    AgentAction,\n",
    "    AgentFinish,\n",
    "    StructuredChatOutputParser,\n",
    ")\n",
    "from langchain.chains.openai_functions.base import convert_to_openai_function\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser, parse_json_markdown\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.tools import tool\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_benchmarks.schema import BaseTask, RegisteredModel\n",
    "from langchain_benchmarks.tool_usage import apply_agent_executor_adapter\n",
    "from langchain_benchmarks.tool_usage.agents import apply_agent_executor_adapter\n",
    "\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str) -> str:\n",
    "    \"\"\"The final answer to the question.\"\"\"\n",
    "    return answer\n",
    "\n",
    "\n",
    "def extract_first_json_object(text):\n",
    "    # A hacky FSM to get the first JSON object across newlines\n",
    "    OUTSIDE, INSIDE, IN_STRING = range(3)\n",
    "\n",
    "    state = OUTSIDE\n",
    "    nested_level = 0\n",
    "    start_index = None\n",
    "\n",
    "    def is_escaped(index):\n",
    "        escape = False\n",
    "        while index > 0 and text[index - 1] == \"\\\\\":\n",
    "            escape = not escape\n",
    "            index -= 1\n",
    "        return escape\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if state == OUTSIDE:\n",
    "            if char == \"{\":\n",
    "                state = INSIDE\n",
    "                nested_level = 1\n",
    "                start_index = i\n",
    "\n",
    "        elif state == INSIDE:\n",
    "            if char == '\"' and not is_escaped(i):\n",
    "                state = IN_STRING\n",
    "            elif char == \"{\":\n",
    "                nested_level += 1\n",
    "            elif char == \"}\":\n",
    "                nested_level -= 1\n",
    "                if nested_level == 0:\n",
    "                    return text[start_index : i + 1]\n",
    "\n",
    "        elif state == IN_STRING:\n",
    "            if char == '\"' and not is_escaped(i):\n",
    "                state = INSIDE\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse(message, prefix: str = \"\") -> dict:\n",
    "    content = prefix + message.content.replace(\"\\_\", \"_\")\n",
    "    content = extract_first_json_object(content)\n",
    "    try:\n",
    "        response = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        response = parse_json_markdown(content)\n",
    "    if response[\"action\"] == \"final_answer\":\n",
    "        return AgentFinish({\"output\": response[\"action_input\"]}, content)\n",
    "    else:\n",
    "        return AgentAction(\n",
    "            response[\"action\"],\n",
    "            response.get(\"action_input\", {}),\n",
    "            content,\n",
    "        )\n",
    "\n",
    "\n",
    "def format_intermediate_steps(\n",
    "    intermediate_steps: Sequence[Tuple[AgentAction, str]],\n",
    ") -> str:\n",
    "    if not intermediate_steps:\n",
    "        return \"\"\n",
    "\n",
    "    # response_tmpl = \"{action}\\n{{\\\"response\\\": \\\"{observation}\\\"}}\"\n",
    "    response_tmpl = \"{action}\\n# Returned {observation}\"\n",
    "    serialized = \"\\n\".join(\n",
    "        [\n",
    "            # f\"{agent_action.log.strip()}\\n{{\\\"response\\\": \\\"{observation}\\\"}}\"\n",
    "            response_tmpl.format(\n",
    "                action=agent_action.log.strip(), observation=observation\n",
    "            )\n",
    "            for agent_action, observation in intermediate_steps\n",
    "        ]\n",
    "    )\n",
    "    return f\"\"\"\n",
    "```log.txt\n",
    "{serialized}\n",
    "```\n",
    "Consider previous steps above. What's your next step?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_scratchpad(x):\n",
    "    intermediate_steps = x[\"intermediate_steps\"]\n",
    "    return format_intermediate_steps(intermediate_steps)\n",
    "\n",
    "\n",
    "class AgentFactory:\n",
    "    def __init__(\n",
    "        self, task: BaseTask, model: RegisteredModel, num_retries: int = 5\n",
    "    ) -> None:\n",
    "        self.task = task\n",
    "        self.model = model\n",
    "        self.num_retries = num_retries\n",
    "\n",
    "    def create_this_ugly_thing(self, env):\n",
    "        tools = env.tools\n",
    "\n",
    "        # schemas = []\n",
    "        # for tool in tools + [final_answer]:\n",
    "        #     function_def = convert_to_openai_function(tool.args_schema)\n",
    "        #     function_def[\"name\"] = tool.name\n",
    "        #     schemas.append(function_def)\n",
    "        # tools_str = \"\\n\".join([json.dumps(sc) for sc in schemas])\n",
    "        tools_str = \"\\n\".join([tool.description for tool in tools + [final_answer]])\n",
    "        messages = [\n",
    "            (\n",
    "                \"system\",\n",
    "                f\"Task Instructions: {self.task.instructions}\\n\\n\"\n",
    "                \"The following tools are exposed via an API:\\n\"\n",
    "                \"{tools}\\n\\n\"\n",
    "                \"Respond with one JSONL line to make your next action and call the API of a single tool.\"\n",
    "                \"\"\" Format invocations like this:\n",
    "{{\"action\": \"tool name\",\"action_input\": {{TOOL BODY}}}}\n",
    "\\n\\nUse the final_answer tool only once you know the correct answer and have called the tools required for the task.\"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"{input}{agent_scratchpad}\\n\\nNote: Remember to respond in 1 JSONL line.\",\n",
    "            ),\n",
    "        ]\n",
    "        parse_fn = parse\n",
    "        if self.model.type == \"llm\":\n",
    "            messages += [(\"assistant\", \"{{\")]\n",
    "            # Fill it back in\n",
    "            parse_fn = partial(parse_fn, prefix=\"{\")\n",
    "        prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        prompt = prompt.partial(tools=tools_str)\n",
    "\n",
    "        llm = self.model.get_model(model_params={\"temperature\": 0}).bind(stop=[\"\\n\\n\"])\n",
    "        if self.num_retries:\n",
    "            llm = llm.with_retry(stop_after_attempt=self.num_retries)\n",
    "\n",
    "        @RunnableLambda\n",
    "        def empty_fallback(x):\n",
    "            \"\"\"Return an empty response to avoid misleading metrics.\"\"\"\n",
    "            return {\n",
    "                \"intermediate_steps\": [],\n",
    "                \"state\": None,\n",
    "                \"output\": \"ERROR\",\n",
    "            }\n",
    "\n",
    "        agent = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"agent_scratchpad\": format_scratchpad,\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "            | parse_fn\n",
    "        )\n",
    "\n",
    "        return AgentExecutor(\n",
    "            agent=agent, tools=tools, return_intermediate_steps=True\n",
    "        ).with_fallbacks([empty_fallback])\n",
    "\n",
    "    def __call__(self):\n",
    "        # This factory creates a new environment for every agent run.\n",
    "        # The reason is that the environment may be associated with an environment state (e.g., typewriter)\n",
    "        # which is changed by the actions of the agent.\n",
    "        # At the end of the run, the environment state will be read.\n",
    "        env = self.task.create_environment()\n",
    "        executor = self.create_this_ugly_thing(env)\n",
    "        # Apply the adapters so that inputs and outputs match dataset schema\n",
    "        # state_reader automatically adds the state of the environment at the end of the run.\n",
    "        return apply_agent_executor_adapter(executor, state_reader=env.read_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821e4b0-8e67-418a-840c-470fcde42df0",
   "metadata": {},
   "source": [
    "## Eval\n",
    "\n",
    "Let's evaluate an agent now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd6cead0-3c37-4a73-8795-7819220797ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_benchmarks.model_registration import model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb32763c-79ab-426a-8fc6-bf8ebb0dd432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[------->                                          ] 3/20View the evaluation results for project 'mixtral-8x7b-fw-chat-ece3-Tool Usage - Typewriter (1 tool)' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/82ca6840-cf23-4bb0-a9be-55237ebbe9d3/compare?selectedSessions=2b92de52-2830-40cb-a396-4c08e0bf1c9b\n",
      "\n",
      "View all tests for Dataset Tool Usage - Typewriter (1 tool) at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/82ca6840-cf23-4bb0-a9be-55237ebbe9d3\n",
      "[------------------------------------------------->] 20/20\n",
      "View the evaluation results for project 'mixtral-8x7b-ece3-Tool Usage - Typewriter (1 tool)' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/82ca6840-cf23-4bb0-a9be-55237ebbe9d3/compare?selectedSessions=ff797831-aee8-43db-a814-7727f9240006\n",
      "\n",
      "View all tests for Dataset Tool Usage - Typewriter (1 tool) at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/82ca6840-cf23-4bb0-a9be-55237ebbe9d3\n",
      "[------------------------------------------------->] 20/20\n",
      "View the evaluation results for project 'mixtral-8x7b-fw-chat-ece3-Tool Usage - Typewriter (26 tools)' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/2f462c7a-f9b9-46e7-b96b-7469e965f478/compare?selectedSessions=1adbc135-93d9-46b2-a33a-e5470eded263\n",
      "\n",
      "View all tests for Dataset Tool Usage - Typewriter (26 tools) at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/2f462c7a-f9b9-46e7-b96b-7469e965f478\n",
      "[------------------------------------------------->] 20/20\n",
      "View the evaluation results for project 'mixtral-8x7b-ece3-Tool Usage - Typewriter (26 tools)' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/2f462c7a-f9b9-46e7-b96b-7469e965f478/compare?selectedSessions=a8548cef-4afd-4f7e-9d21-7bd2fb3f9033\n",
      "\n",
      "View all tests for Dataset Tool Usage - Typewriter (26 tools) at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/2f462c7a-f9b9-46e7-b96b-7469e965f478\n",
      "[------------------------------------------------->] 20/20\n",
      "View the evaluation results for project 'mixtral-8x7b-fw-chat-ece3-Tool Usage - Relational Data' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826/compare?selectedSessions=685df1fb-605d-40e3-b645-ae132a0a6229\n",
      "\n",
      "View all tests for Dataset Tool Usage - Relational Data at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826\n",
      "[------------------------------------------------->] 21/21\n",
      "View the evaluation results for project 'mixtral-8x7b-ece3-Tool Usage - Relational Data' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826/compare?selectedSessions=bb4d1ee4-bbc8-4969-a4f0-2b0732444785\n",
      "\n",
      "View all tests for Dataset Tool Usage - Relational Data at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/df6be6c9-05b3-445e-8836-ebb4aba63826\n",
      "[------------------------------------------------->] 21/21\n",
      "View the evaluation results for project 'mixtral-8x7b-fw-chat-ece3-Multiverse Math' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/108bdc68-1808-4b60-92ef-fbd9bd7e1ad0/compare?selectedSessions=ac7ec5aa-108d-4c5b-9c30-8e954fa132aa\n",
      "\n",
      "View all tests for Dataset Multiverse Math at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/108bdc68-1808-4b60-92ef-fbd9bd7e1ad0\n",
      "[------------------------------------------------->] 10/10\n",
      "View the evaluation results for project 'mixtral-8x7b-ece3-Multiverse Math' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/108bdc68-1808-4b60-92ef-fbd9bd7e1ad0/compare?selectedSessions=9d8573ee-847f-400a-8894-2e77c62e76ab\n",
      "\n",
      "View all tests for Dataset Multiverse Math at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/108bdc68-1808-4b60-92ef-fbd9bd7e1ad0\n",
      "[------------------------------------------------->] 10/10"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks.tool_usage import get_eval_config\n",
    "\n",
    "experiment_uuid = uuid.uuid4().hex[:4]\n",
    "\n",
    "client = Client()\n",
    "\n",
    "task_names = [task.name for task in registry.filter(Type=\"ToolUsageTask\")]\n",
    "models = [\"mixtral-8x7b-fw-chat\", \"mixtral-8x7b\"]\n",
    "\n",
    "for task_name in task_names:\n",
    "    for model_name in models:\n",
    "        print()\n",
    "        model = model_registry[model_name]\n",
    "        task = registry[task_name]\n",
    "        clone_public_dataset(task.dataset_id, dataset_name=task.name)\n",
    "        eval_config = task.get_eval_config()\n",
    "        test_run = client.run_on_dataset(\n",
    "            dataset_name=task.name,\n",
    "            llm_or_chain_factory=AgentFactory(task, model),\n",
    "            evaluation=eval_config,\n",
    "            project_name=f\"{model.name}-{experiment_uuid}-{task.name}\",\n",
    "            tags=[model.name],\n",
    "            project_metadata={\"id\": experiment_uuid, **model.params},\n",
    "            verbose=True,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
