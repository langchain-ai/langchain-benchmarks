{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba9f105-c48f-4d8c-8253-355ef13156b0",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "Let's benchmark against all tool usage tasks. \n",
    "\n",
    "Expand the models list to benchmark with different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86ba232-db1c-462c-9379-2a3c9a60c91d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a7483b-d08f-49fa-83da-619863171e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from langsmith.client import Client\n",
    "\n",
    "from langchain_benchmarks import (\n",
    "    __version__,\n",
    "    clone_public_dataset,\n",
    "    model_registry,\n",
    "    registry,\n",
    ")\n",
    "\n",
    "from langchain_benchmarks.tool_usage.agents import (\n",
    "    StandardAgentFactory\n",
    ")\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbe23b-a3b1-4607-929d-ea6e88b7085e",
   "metadata": {},
   "source": [
    "Prior to starting the tests, you may want to verify\n",
    "that the task that you're working with and the models are propelry defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f2bb0c-c741-4fb4-96bc-54b3ee88bf5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = registry[\"Multiverse Math\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3421deea-10b2-4a51-82b3-9127ca1d3a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are requested to solve math questions in an alternate mathematical universe. The operations have been altered to yield different results than expected. Do not guess the answer or rely on your  innate knowledge of math. Use the provided tools to answer the question. While associativity and commutativity apply, distributivity does not. Answer the question using the fewest possible tools. Only include the numeric response without any clarifications.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8afa6250-0771-4762-9d83-ceddbfcce6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"{instructions}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "factory = StandardAgentFactory(task, model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b2edff-006f-4910-8254-0bd9f0cf0da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/src/langchain/libs/partners/anthropic/langchain_anthropic/chat_models.py:347: UserWarning: stream: Tool use is not yet supported in streaming mode.\n",
      "  warnings.warn(\"stream: Tool use is not yet supported in streaming mode.\")\n",
      "/home/eugene/src/langchain/libs/partners/anthropic/langchain_anthropic/chat_models.py:347: UserWarning: stream: Tool use is not yet supported in streaming mode.\n",
      "  warnings.warn(\"stream: Tool use is not yet supported in streaming mode.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 132*232',\n",
       " 'output': [],\n",
       " 'intermediate_steps': [(ToolAgentAction(tool='multiply', tool_input={'a': 132, 'b': 232}, log=\"\\nInvoking: `multiply` with `{'a': 132, 'b': 232}`\\nresponded: [{'text': '<thinking>\\\\nThe relevant tool to answer this request is multiply, since the user is asking to multiply two numbers together.\\\\n\\\\nThe multiply function requires two parameters:\\\\na (number): The user provided the value 132\\\\nb (number): The user provided the value 232\\\\n\\\\nSince the user provided values for both required parameters, I can proceed with the multiply function call to answer the question.\\\\n</thinking>', 'type': 'text'}, {'id': 'toolu_01FuWpKNrXQp3kZfbdsxjwdd', 'input': {'a': 132, 'b': 232}, 'name': 'multiply', 'type': 'tool_use'}]\\n\\n\", message_log=[AIMessageChunk(content=[{'text': '<thinking>\\nThe relevant tool to answer this request is multiply, since the user is asking to multiply two numbers together.\\n\\nThe multiply function requires two parameters:\\na (number): The user provided the value 132\\nb (number): The user provided the value 232\\n\\nSince the user provided values for both required parameters, I can proceed with the multiply function call to answer the question.\\n</thinking>', 'type': 'text'}, {'id': 'toolu_01FuWpKNrXQp3kZfbdsxjwdd', 'input': {'a': 132, 'b': 232}, 'name': 'multiply', 'type': 'tool_use'}], id='run-3b31d102-2aa3-4ca7-ba37-1dee69c16c5d', tool_calls=[{'name': 'multiply', 'args': {'a': 132, 'b': 232}, 'id': 'toolu_01FuWpKNrXQp3kZfbdsxjwdd'}], tool_call_chunks=[{'name': 'multiply', 'args': '{\"a\": 132, \"b\": 232}', 'id': 'toolu_01FuWpKNrXQp3kZfbdsxjwdd', 'index': 0}])], tool_call_id='toolu_01FuWpKNrXQp3kZfbdsxjwdd'),\n",
       "   33686.4)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory().invoke({\"question\": \"what is 132*232\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28d470-0868-4a1c-b571-0f2ef12250de",
   "metadata": {},
   "source": [
    "# OUT OF DATE (BELOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165f3a1-4e70-4caa-b082-78d4e0c56410",
   "metadata": {},
   "source": [
    "Let's make an experiment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066d7695-416c-4faf-8c33-c40e5f136672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_uuid = \"meow\"  # Or generate ranom using uuid.uuid4().hex[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bde1e0-f9f7-47f1-816a-d01a28577544",
   "metadata": {},
   "source": [
    "Define the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d390b6-9ade-424c-aabb-d450f52ed121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tests = [\n",
    "    # 2-tuple of (architecture, model name)\n",
    "    # (\"anthropic_tool_user\", \"claude-2.1\"),\n",
    "    (\"openai_functions\", \"mistral-7b-instruct-v0.1\"),\n",
    "    # (\"openai_functions\", \"gpt-3.5-turbo-1106\"),\n",
    "    # (\"openai_functions\", \"gpt-3.5-turbo-0613\"),\n",
    "    # (\"openai_functions\", \"gpt-4-1106-preview\"),\n",
    "    # (\"openai_functions\", \"gpt-4-0613\"),\n",
    "    # (\"openai_assistant\", \"gpt-4-1106-preview\"),\n",
    "    # (\"openai_assistant\", \"gpt-3.5-turbo-1106\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b7c24-8b4d-4bd7-8b00-365fbe61897f",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a1894b-6232-421c-a243-567617cba083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fbc3ef-7a3f-430f-8b79-45af5861b3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Tool Usage - Typewriter (26 tools) already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/0c8a0acd-0308-4298-82bb-e28cec0ca5e1.\n",
      "\n",
      "Benchmarking Tool Usage - Typewriter (26 tools) with model: mistral-7b-instruct-v0.1 and arch: openai_functions\n",
      "View the evaluation results for project 'mistral-7b-instruct-v0.1-Tool Usage - Typewriter (26 tools)-2023-12-18-woof' at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/0c8a0acd-0308-4298-82bb-e28cec0ca5e1/compare?selectedSessions=7e9ac2e4-1d33-49bc-9694-f2db6d2e6243\n",
      "\n",
      "View all tests for Dataset Tool Usage - Typewriter (26 tools) at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/0c8a0acd-0308-4298-82bb-e28cec0ca5e1\n",
      "[>                                                 ] 0/20--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "--- Formatting open ai tool messages ---\n",
      "Input: [(OpenAIToolAgentAction(tool='a', tool_input={}, log='\\nInvoking: `a` with `{}`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_e57db65ebbe54a34a940cdbb86ff3624', 'function': {'arguments': '{}', 'name': 'a'}, 'type': 'function'}]})], tool_call_id='call_e57db65ebbe54a34a940cdbb86ff3624'), 'OK')]\n",
      "Final messages: [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_e57db65ebbe54a34a940cdbb86ff3624', 'function': {'arguments': '{}', 'name': 'a'}, 'type': 'function'}]}), ToolMessage(content='OK', additional_kwargs={'name': 'a'}, tool_call_id='call_e57db65ebbe54a34a940cdbb86ff3624')]\n",
      "[->                                                ] 1/20--- Formatting open ai tool messages ---\n",
      "Input: [(OpenAIToolAgentAction(tool='a', tool_input={}, log='\\nInvoking: `a` with `{}`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f81d6529932d491f9130d2778aa250a7', 'function': {'arguments': '{}', 'name': 'a'}, 'type': 'function'}]})], tool_call_id='call_f81d6529932d491f9130d2778aa250a7'), 'OK')]\n",
      "Final messages: [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f81d6529932d491f9130d2778aa250a7', 'function': {'arguments': '{}', 'name': 'a'}, 'type': 'function'}]}), ToolMessage(content='OK', additional_kwargs={'name': 'a'}, tool_call_id='call_f81d6529932d491f9130d2778aa250a7')]\n",
      "--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "[---->                                             ] 2/20--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "[------->                                          ] 3/20--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "--- Formatting open ai tool messages ---\n",
      "Input: [(OpenAIToolAgentAction(tool='a', tool_input={}, log='\\nInvoking: `a` with `{}`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_a85021e20b5840138be6683edbbdb83c', 'function': {'arguments': '{}', 'name': 'a'}, 'type': 'function'}]})], tool_call_id='call_a85021e20b5840138be6683edbbdb83c'), 'OK')]\n",
      "Final messages: [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_a85021e20b5840138be6683edbbdb83c', 'function': {'arguments': '{}', 'name': 'a'}, 'type': 'function'}]}), ToolMessage(content='OK', additional_kwargs={'name': 'a'}, tool_call_id='call_a85021e20b5840138be6683edbbdb83c')]\n",
      "[--------->                                        ] 4/20--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "[----------->                                      ] 5/20--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n",
      "[-------------->                                   ] 6/20--- Formatting open ai tool messages ---\n",
      "Input: []\n",
      "Final messages: []\n"
     ]
    }
   ],
   "source": [
    "client = Client()  # Launch langsmith client for cloning datasets\n",
    "today = datetime.date.today().isoformat()\n",
    "rate_limiter = RateLimiter(requests_per_second=0.5)\n",
    "\n",
    "for task in registry:\n",
    "    if task.type != \"ToolUsageTask\":\n",
    "        continue\n",
    "\n",
    "    if \"26\" not in task.name:\n",
    "        continue\n",
    "\n",
    "    dataset_name = task.name\n",
    "    clone_public_dataset(task.dataset_id, dataset_name=dataset_name)\n",
    "\n",
    "    for arch, model in tests:\n",
    "        print()\n",
    "        print(f\"Benchmarking {task.name} with model: {model} and arch: {arch}\")\n",
    "        eval_config = task.get_eval_config()\n",
    "\n",
    "        if arch == \"openai_functions\":\n",
    "            agent_factory = OpenAIAgentFactory(\n",
    "                task, model=model, rate_limiter=rate_limiter\n",
    "            )\n",
    "        elif arch == \"custom_agent\":\n",
    "            agent_factory = CustomAgentFactory(\n",
    "                task, model=model, rate_limiter=rate_limiter\n",
    "            )\n",
    "        elif arch == \"anthropic_tool_user\":\n",
    "            agent_factory = AnthropicToolUserFactory(task)\n",
    "        elif arch == \"openai_assistant\":\n",
    "            agent_factory = OpenAIAssistantFactory(task, model=model)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        client.run_on_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            llm_or_chain_factory=agent_factory,\n",
    "            evaluation=eval_config,\n",
    "            verbose=False,\n",
    "            project_name=f\"{model}-{task.name}-{today}-{experiment_uuid}\",\n",
    "            tags=[model],\n",
    "            concurrency_level=3,\n",
    "            project_metadata={\n",
    "                \"model\": model,\n",
    "                \"id\": experiment_uuid,\n",
    "                \"task\": task.name,\n",
    "                \"date\": today,\n",
    "                \"langchain_benchmarks_version\": __version__,\n",
    "                \"arch\": arch,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7999f-e8ab-45a6-88a9-0ae76f3d24cf",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818572a-a5fb-4153-bbe0-6f9e90813a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langsmith.client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7890951-ffde-4706-95e5-ae3e9bf0e8a6",
   "metadata": {},
   "source": [
    "Let's fetch all the data that has the same experiment ID and place it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44822aa4-8c4e-46be-8126-b79a9acdf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ids = [\"3f3e\"]\n",
    "\n",
    "\n",
    "def _endswith(s, suffixes):\n",
    "    return any(s.endswith(suffix) for suffix in suffixes)\n",
    "\n",
    "\n",
    "client = Client()\n",
    "projects = [\n",
    "    project\n",
    "    for project in client.list_projects()\n",
    "    if _endswith(project.name, experiment_ids)\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for project in projects:\n",
    "    # Temporary way to get tag information\n",
    "    project_info = client.read_project(project_id=project.id)\n",
    "    try:\n",
    "        test_results = client.get_test_results(project_name=project.name)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    for k, v in project_info.extra[\"metadata\"].items():\n",
    "        test_results[k] = v\n",
    "\n",
    "    dfs.append(test_results)\n",
    "\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065b7a0-d514-49f7-9d79-67181c41f56d",
   "metadata": {},
   "source": [
    "Compute a standardized \"correct\" column. It uses \"Correct Final State\" for tool usage tasks, and \"correctness (which is based on output) for the other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0466a-25f4-44d7-bd2a-20da51461994",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "\n",
    "for r in df.to_dict(orient=\"records\"):\n",
    "    if \"Typewriter\" in r[\"task\"]:\n",
    "        correct.append(r[\"feedback.Correct Final State\"])\n",
    "    else:\n",
    "        correct.append(r[\"feedback.correctness\"])\n",
    "\n",
    "df[\"correct\"] = correct\n",
    "df[\"correct\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b8ae9-c84b-4ebc-88ab-fa0ac5e28a57",
   "metadata": {},
   "source": [
    "Compute some statistics. We're using estimating standard error of the mean assuming a bernoulli process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d080c-d3ac-43c3-a527-9961913db2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = df.groupby([\"model\", \"task\"])[\"correct\"].sum().to_frame(\"num_correct\")\n",
    "total = df.groupby([\"task\", \"model\"]).size().to_frame(\"total\")\n",
    "stats_df = total.join(num_correct)\n",
    "stats_df[\"% correct\"] = stats_df[\"num_correct\"] / stats_df[\"total\"]\n",
    "stats_df[\"error\"] = np.sqrt(\n",
    "    stats_df[\"% correct\"] * (1 - stats_df[\"% correct\"]) / stats_df[\"total\"]\n",
    ")\n",
    "\n",
    "# stats_df\n",
    "\n",
    "models = [\n",
    "    \"llama-v2-70b-chat-fw\",\n",
    "    \"mixtral-8x7b-instruct-fw\",\n",
    "    \"claude-2\",\n",
    "    \"claude-2.1\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo-1106\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4-1106-preview\",\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"Tool Usage - Typewriter (1 tool)\",\n",
    "    \"Tool Usage - Typewriter (26 tools)\",\n",
    "    \"Multiverse Math\",\n",
    "    \"Tool Usage - Relational Data\",\n",
    "]\n",
    "\n",
    "stats_df = stats_df.reset_index()\n",
    "stats_df = stats_df[stats_df[\"model\"].isin(models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f79af-128c-4e2e-8c1e-807e397b9791",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df66a1-960c-40a3-abc8-58b503fceda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(tasks))  # the label locations\n",
    "width = 0.08  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout=\"constrained\", figsize=(16, 4))\n",
    "colormap = plt.get_cmap(\"Set2\").colors\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    try:\n",
    "        results = stats_df.set_index(\"model\").loc[model]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    color = colormap[idx]\n",
    "\n",
    "    results = results.set_index(\"task\").loc[tasks]\n",
    "    measurement = results[\"% correct\"]\n",
    "\n",
    "    values = [round(m, 2) for m in measurement]\n",
    "\n",
    "    offset = width * multiplier * 1.4\n",
    "    rects = ax.bar(\n",
    "        x + offset, values, width, label=model, yerr=results[\"error\"], color=color\n",
    "    )\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel(\"% Questions Answered Correctly\")\n",
    "ax.set_title(\"Tool Usage Performance\")\n",
    "ax.set_xticks(x + width + 0.3, tasks)\n",
    "ax.legend(\n",
    "    loc=\"center left\", ncols=1, bbox_to_anchor=(1.0, 0.5), frameon=False, title=\"Model\"\n",
    ")\n",
    "ax.set_ylim(0, 1.10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
