{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd0617a-4d00-4c4c-a5df-abc3430e7897",
   "metadata": {},
   "source": [
    "# Semi-structured eval: Multi vector\n",
    "\n",
    "`Semi-structured Reports` is a public dataset that contains question-answer pairs from documents with text and tables.\n",
    "\n",
    "The question-answer pairs are derived from the tables as well as some of the paragraphs in the docs.\n",
    "\n",
    "We evaluation performance using multi-vector retriever for RAG. \n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd540d-705f-4042-9ed0-aee42d29f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langsmith langchain_benchmarks\n",
    "# %pip install --quiet chromadb openai pypdf tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29031433-53db-43bb-ab1a-8ac1721661e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "env_vars = [\"LANGCHAIN_API_KEY\", \"OPENAI_API_KEY\"]\n",
    "for var in env_vars:\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass.getpass(prompt=f\"Enter your {var}: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560e044-f5ac-418b-b3d6-164b423ab23b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Fetch the associated PDFs from remote cache for the dataset so that we can perform ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f8b0e3-693a-4eed-98e7-c0fa9ba02ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "from langchain_benchmarks.rag.tasks.semi_structured_reports import get_file_names\n",
    "\n",
    "# Task\n",
    "task = registry[\"Semi-structured Reports\"]\n",
    "\n",
    "# Files used\n",
    "paths = list(get_file_names())\n",
    "files = [str(p) for p in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720016d6-9206-4560-9b12-5881dbcabeb3",
   "metadata": {},
   "source": [
    "Clone the dataset so that it's available in our LangSmith datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2309e4-0b35-477b-80a6-d4cb06ca4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_public_dataset(task.dataset_id, dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1db618-05c4-4253-a54b-1c554dd0dc78",
   "metadata": {},
   "source": [
    "## Load and index\n",
    "\n",
    "We build a retriever that focuses on tables. \n",
    "\n",
    "To do this, we use an LLM to scan each page and summarize any tables within the page. \n",
    "\n",
    "We then index those summaries for retrieval and store the raw page text containing the table with [multi-vector retriever](https://blog.langchain.dev/semi-structured-multi-modal-rag/). \n",
    "\n",
    "Finally, we use [ensemble retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/ensemble) to mix retrieved table chunks with the raw text chunks: \n",
    "\n",
    "* Combines the rankings from different retrievers into a single, unified ranking.\n",
    "* Each retriever provides a list of documents (or search results) ranked based on their relevance to the query.\n",
    "* The weights represent the relative importance or trust you place in each retriever's results.\n",
    "* The weights are used to scale the contribution of each retriever to the final combined ranking.\n",
    "* The RRF method uses the rank of each item in the lists provided by the retrievers.\n",
    "* The basic idea is to give higher scores to items that are ranked higher (i.e., have a lower rank number) in the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14be7d-30c8-4084-afad-3e82c3fbf9e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "def prepare_documents(docs):\n",
    "    \"\"\"\n",
    "    Prepare documents for prompt. Concatenates Document objects (after extracting their page_content)\n",
    "    and strings into a single string, separated by two newlines.\n",
    "\n",
    "    :param docs: A list of str or Document objects.\n",
    "    :return: A single string containing all documents.\n",
    "    \"\"\"\n",
    "    # Process each document and append it to the list\n",
    "    processed_docs = [\n",
    "        doc.page_content if isinstance(doc, Document) else doc for doc in docs\n",
    "    ]\n",
    "\n",
    "    # Join all processed documents into a single string\n",
    "    return \"\\n\\n\".join(processed_docs)\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    add_documents(retriever, text_summaries, texts)\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def generate_doc_summary(file):\n",
    "    \"\"\"\n",
    "    Create a doc summary\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked extracting two attributes \\\n",
    "    from financial documents. (1) Tell me the company that the document is \\\n",
    "    focused on. (2) Look at any tables in the document and tell me the units \\ \n",
    "    of the table. Many table will have '(In thousands)' or '(in millions)' prior \\\n",
    "    to the table text. Provide these two for the document: \\n\\n {document} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")\n",
    "    summarize_chain = {\"document\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Load doc\n",
    "    loader = PyPDFLoader(file)\n",
    "    pdf_pages = loader.load()\n",
    "    texts = [t.page_content for t in pdf_pages]\n",
    "    text_string = \" \".join(texts)\n",
    "    summary = summarize_chain.invoke({\"document\": text_string})\n",
    "    return summary\n",
    "\n",
    "\n",
    "def generate_table_summaries(texts):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables within a provided text chunk. \\\n",
    "    If the text chunk contains tables, then give a brief summary of the table and list the row and column \\\n",
    "    names to identify what is captured in the table. Do not sumnmarize quantitative results in the table. \\ \n",
    "    If there is no table present, then just return \"No table\". \\n\\n Text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries\n",
    "\n",
    "\n",
    "def load_and_split(file, token_count, split_document=True):\n",
    "    \"\"\"\n",
    "    Load and optionally split PDF files.\n",
    "\n",
    "    Args:\n",
    "        file (str): File path.\n",
    "        token_count (int): Token count for splitting.\n",
    "        split_document (bool): Flag for splitting or returning pages.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = PyPDFLoader(file)\n",
    "    pdf_pages = loader.load()\n",
    "\n",
    "    if split_document:\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=token_count, chunk_overlap=50\n",
    "        )\n",
    "\n",
    "        docs = text_splitter.split_documents(pdf_pages)\n",
    "        texts = [d.page_content for d in docs]\n",
    "    else:\n",
    "        texts = [d.page_content for d in pdf_pages]\n",
    "\n",
    "    print(f\"There are {len(texts)} text elements\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_files(files, token_count, split_document):\n",
    "    \"\"\"\n",
    "    Load files.\n",
    "\n",
    "    Args:\n",
    "        files (list): List of file names.\n",
    "        dir (str): Directory path.\n",
    "        token_count (int): Token count for splitting.\n",
    "        split_document (bool): Flag for splitting documents.\n",
    "    \"\"\"\n",
    "\n",
    "    texts = []\n",
    "    for fi in files:\n",
    "        doc_summary = generate_doc_summary(fi)\n",
    "        texts.extend(load_and_split(fi, token_count, split_document))\n",
    "    return texts, doc_summary\n",
    "\n",
    "\n",
    "def rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    RAG chain.\n",
    "\n",
    "    Args:\n",
    "        retriever: The retriever to use.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(prepare_documents),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Experiment configurations\n",
    "experiments = [\n",
    "    (None, False, \"page_split_multivector\"),\n",
    "]\n",
    "\n",
    "# Run\n",
    "stor_chain = {}\n",
    "for token_count, split_document, expt in experiments:\n",
    "    # Get texts and doc summary\n",
    "    doc_texts, doc_summary = load_files(files, token_count, split_document)\n",
    "\n",
    "    # Get table summaries\n",
    "    doc_table_summaries = generate_table_summaries(doc_texts)\n",
    "\n",
    "    # Add doc summary to table summary to preserve context\n",
    "    doc_text_summaries = [\n",
    "        \"Here is a summary of the doc: \\n\\n\"\n",
    "        + doc_summary\n",
    "        + \"\\n\\n Here is a summary of a table within this doc: \\n\\n\"\n",
    "        + t\n",
    "        for t in doc_table_summaries\n",
    "    ]\n",
    "\n",
    "    # The vectorstore to use to index the summaries\n",
    "    vectorstore = Chroma(collection_name=expt, embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "    # Create our table retriever\n",
    "    table_retriever = create_multi_vector_retriever(\n",
    "        vectorstore, doc_table_summaries, doc_texts\n",
    "    )\n",
    "\n",
    "    # Create our docs retriever\n",
    "    vectorstore_docs = Chroma.from_texts(\n",
    "        texts=doc_texts, collection_name=expt + \"docs\", embedding=OpenAIEmbeddings()\n",
    "    )\n",
    "    docs_retriever = vectorstore_docs.as_retriever()\n",
    "\n",
    "    # Initialize ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[table_retriever, docs_retriever], weights=[0.75, 0.25]\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    stor_chain[expt] = rag_chain(ensemble_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aeb2e2-156d-4a39-be93-4f401f1df455",
   "metadata": {},
   "source": [
    "## Eval\n",
    "\n",
    "Run eval onm our dataset, `Semi-structured Reports`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd91b5-6b8e-4bb5-b97a-42ccc5dd53dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.client import Client\n",
    "\n",
    "# Config\n",
    "client = Client()\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"page_split_multivector_emsemble\": stor_chain[\"page_split_multivector\"],\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "run_id = uuid.uuid4().hex[:4]\n",
    "test_runs = {}\n",
    "for project_name, chain in chain_map.items():\n",
    "    test_runs[project_name] = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=lambda: (lambda x: x[\"Question\"]) | chain,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=f\"{run_id}-{project_name}\",\n",
    "        project_metadata={\"chain\": project_name},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
